{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chart_2_text_reduced.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "B92qnMW1Oqmi",
        "TivywSYvLxHL",
        "-IInXlLqLOAY",
        "Kh2g0_NYLVCH",
        "XB1A-1yiEAtx",
        "p7dA-aZzED5b"
      ],
      "authorship_tag": "ABX9TyMiKzv54WTbkgamE2XDvNCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliasKng/BT-Code/blob/master/Chart_2_text_reduced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3mHL9GTjbPL"
      },
      "source": [
        "#Chart-2-text-reduced\n",
        "\n",
        "This notebook will provide the functionality of chart-to-text, however, for single value inputs.\n",
        "\n",
        "So it will do the data-preparation and then put the values into the model and return the summary for the chart.\n",
        "\n",
        "The goal is to provide a function:\n",
        "\n",
        " **createSummary(chartData: ChartData): string**\n",
        "\n",
        "where the returned string is the summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EceSAKtf8Udu"
      },
      "source": [
        "## Startup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B92qnMW1Oqmi"
      },
      "source": [
        "### Installations & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaLPeoJzSelX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72339cfe-dbd0-4c3a-b62a-d3a21b0a8821"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "% cd /content/gdrive/MyDrive/BA-Code/\n",
        "\n",
        "! pip install -U spacy\n",
        "! python3 -m spacy download en_core_web_md\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from typing import List\n",
        "import spacy\n",
        "import io\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "from logging import getLogger\n",
        "import math\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/BA-Code\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting en-core-web-md==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 45.7 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1XNOGTxJO4p"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TivywSYvLxHL"
      },
      "source": [
        "#### General utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9foa_vyJJN_3"
      },
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def word_tokenize(string: str, language: str = \"english\") -> List[str]:\n",
        "    \"\"\"tokenizes a given string into a list of substrings.\n",
        "\n",
        "    :param string: String to tokenize.\n",
        "    :param language: Language. Either one of ``english'' or ``german''.\n",
        "    \"\"\"\n",
        "    if language not in [\"english\", \"german\"]:\n",
        "        raise ValueError(\"language argument has to be either ``english'' or ``german''\")\n",
        "\n",
        "    # excessive whitespaces\n",
        "    string = re.sub(r\"\\s+\", \" \", string)\n",
        "\n",
        "    # some unicode characters\n",
        "    string = string.replace(\"’\", \"'\")\n",
        "    string = string.replace(\"”\", '\"')\n",
        "    string = string.replace(\"“\", '\"')\n",
        "\n",
        "    # floating point (e.g., 1.3 => 1.3)\n",
        "    string = re.sub(r\"(\\d+)\\.(\\d+)\", r\"\\g<1>._\\g<2>\", string)\n",
        "\n",
        "    # percentage (e.g., below.500 => below .500)\n",
        "    string = re.sub(r\"(\\w+)\\.(\\d+)\", r\"\\g<1> ._\\g<2>\", string)\n",
        "\n",
        "    # end of quote\n",
        "    string = string.replace(\".``\", \". ``\")\n",
        "\n",
        "    # number with apostrophe (e.g. '90)\n",
        "    string = re.sub(r\"\\s'(\\d+)\", r\"' \\g<1>\", string)\n",
        "\n",
        "    # names with Initials (e.g. C. J. Miles)\n",
        "    string = re.sub(r\"(^|\\s)(\\w)\\. (\\w)\\.\", r\"\\g<1>\\g<2>._ \\g<3>._\", string)\n",
        "\n",
        "    # some dots\n",
        "    string = string.replace(\"..\", \" ..\")\n",
        "\n",
        "    # names with apostrophe => expands temporarily\n",
        "    string = re.sub(r\"\\w+'(?!d|s|ll|t|re|ve|\\s)\", r\"\\g<0>_\", string)\n",
        "\n",
        "    # win-loss scores (German notation seems to be XX:YY, but this is also the time format,\n",
        "    # and the times are not tokenized in the original RotoWire. So we manually handle XX:YY\n",
        "    # expression.\n",
        "    string = re.sub(r\"(\\d+)-(\\d+)\", r\"\\g<1> - \\g<2>\", string)\n",
        "\n",
        "    # actual tokenization\n",
        "    tokenized = nltk.word_tokenize(string, language=language)\n",
        "\n",
        "    joined = \" \".join(tokenized)\n",
        "    # shrink expanded name-with-apostrophe expressions\n",
        "    joined = joined.replace(\"'_\", \"'\")\n",
        "    # shrink expanded name-with-initial expressions\n",
        "    joined = joined.replace(\"._\", \".\")\n",
        "    tokenized = joined.split(\" \")\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "def cleanAxisLabel(label):\n",
        "    cleanLabel = re.sub('\\s', '_', label)\n",
        "    cleanLabel = cleanLabel.replace('%', '').replace('*', '')\n",
        "    return cleanLabel\n",
        "  \n",
        "def cleanAxisValue(value):\n",
        "    #print(value)\n",
        "    if value == '-' or value == 'nan':\n",
        "        return '0'\n",
        "    cleanValue = re.sub('\\s', '_', value)\n",
        "    cleanValue = cleanValue.replace('|', '').replace(',', '').replace('%', '').replace('*', '')\n",
        "    return cleanValue\n",
        "\n",
        "def is_number(string):\n",
        "    try:\n",
        "        float(string)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def openMultiColumnData(df):\n",
        "    cols = df.columns\n",
        "    size = df.shape[0]\n",
        "    return cols, size\n",
        "  \n",
        "def getSubject(titleTokens, nerEntities):\n",
        "    fillers = ['in', 'the', 'and', 'or', 'an', 'as', 'can', 'be', 'a', ':', '-',\n",
        "              'to', 'but', 'is', 'of', 'it', 'on', '.', 'at', '(', ')', ',', ';']\n",
        "    entities = {}\n",
        "    entities['Subject'] = []\n",
        "    entities['Date'] = []\n",
        "    # manually find dates, it performs better than using NER\n",
        "    for word in titleTokens:\n",
        "        if word.isnumeric():\n",
        "            if len(word) > 3:\n",
        "                entities['Date'].append(word)\n",
        "        elif word.replace('/', '').isnumeric():\n",
        "            word = word.split('/')[0]\n",
        "            if len(word) > 3:\n",
        "                entities['Date'].append(word)\n",
        "        elif word.replace('-', '').isnumeric():\n",
        "            word = word.split('-')[0]\n",
        "            if len(word) > 3:\n",
        "                entities['Date'].append(word)\n",
        "    # get named entites from title\n",
        "    for X in nerEntities:\n",
        "        if X.label_ == 'GPE' or X.label_ == 'ORG' or X.label_ == 'NORP' or X.label_ == 'LOC':\n",
        "            cleanSubject = [word for word in X.text.split() if word.isalpha() and word not in fillers]\n",
        "            if len(cleanSubject) > 0:\n",
        "                entities['Subject'].append(' '.join(cleanSubject))\n",
        "        if len(entities['Date']) < 1:\n",
        "            if X.label_ == 'DATE':\n",
        "                if X.text.isnumeric():\n",
        "                    entities['Date'].append(X.text)\n",
        "    # guess subject if NER doesn't find one\n",
        "    if len(entities['Subject']) == 0:\n",
        "        uppercaseWords = [word for word in titleTokens if word[0].isupper()]\n",
        "        if len(uppercaseWords) > 1:\n",
        "            guessedSubject = ' '.join(uppercaseWords[1:])\n",
        "        else:\n",
        "            guessedSubject = uppercaseWords[0]\n",
        "        entities['Subject'].append(guessedSubject)\n",
        "    # print(entities['Date'])\n",
        "    cleanTitle = [titleWord for titleWord in titleTokens if titleWord.lower() not in fillers]\n",
        "    return entities, cleanTitle\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IInXlLqLOAY"
      },
      "source": [
        "#### Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4pDkkUbKm1o"
      },
      "source": [
        "logger = getLogger()\n",
        "\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "PAD_WORD = '<pad>'\n",
        "UNK_WORD = '<unk>'\n",
        "\n",
        "SPECIAL_WORD = '<special%i>'\n",
        "SPECIAL_WORDS = 10\n",
        "\n",
        "SEP_WORD = SPECIAL_WORD % 0\n",
        "MASK_WORD = SPECIAL_WORD % 1\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "\n",
        "    def __init__(self, id2word, word2id):\n",
        "        assert len(id2word) == len(word2id)\n",
        "        self.id2word = id2word\n",
        "        self.word2id = word2id\n",
        "        self.bos_index = word2id[BOS_WORD]\n",
        "        self.eos_index = word2id[EOS_WORD]\n",
        "        self.pad_index = word2id[PAD_WORD]\n",
        "        self.unk_index = word2id[UNK_WORD]\n",
        "        self.check_valid()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of words in the dictionary.\n",
        "        \"\"\"\n",
        "        return len(self.id2word)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Returns the word of the specified index.\n",
        "        \"\"\"\n",
        "        return self.id2word[i]\n",
        "\n",
        "    def __contains__(self, w):\n",
        "        \"\"\"\n",
        "        Returns whether a word is in the dictionary.\n",
        "        \"\"\"\n",
        "        return w in self.word2id\n",
        "\n",
        "    def __eq__(self, y):\n",
        "        \"\"\"\n",
        "        Compare this dictionary with another one.\n",
        "        \"\"\"\n",
        "        self.check_valid()\n",
        "        y.check_valid()\n",
        "        if len(self.id2word) != len(y):\n",
        "            return False\n",
        "        return all(self.id2word[i] == y[i] for i in range(len(y)))\n",
        "\n",
        "    def check_valid(self):\n",
        "        \"\"\"\n",
        "        Check that the dictionary is valid.\n",
        "        \"\"\"\n",
        "        assert self.bos_index == 0\n",
        "        assert self.eos_index == 1\n",
        "        assert self.pad_index == 2\n",
        "        assert self.unk_index == 3\n",
        "        assert all(self.id2word[4 + i] == SPECIAL_WORD % i for i in range(SPECIAL_WORDS))\n",
        "        assert len(self.id2word) == len(self.word2id)\n",
        "        for i in range(len(self.id2word)):\n",
        "            assert self.word2id[self.id2word[i]] == i\n",
        "\n",
        "    def index(self, word, no_unk=False):\n",
        "        \"\"\"\n",
        "        Returns the index of the specified word.\n",
        "        \"\"\"\n",
        "        if no_unk:\n",
        "            return self.word2id[word]\n",
        "        else:\n",
        "            return self.word2id.get(word, self.unk_index)\n",
        "\n",
        "    def max_vocab(self, max_vocab):\n",
        "        \"\"\"\n",
        "        Limit the vocabulary size.\n",
        "        \"\"\"\n",
        "        assert max_vocab >= 1\n",
        "        init_size = len(self)\n",
        "        self.id2word = {k: v for k, v in self.id2word.items() if k < max_vocab}\n",
        "        self.word2id = {v: k for k, v in self.id2word.items()}\n",
        "        self.check_valid()\n",
        "        logger.info(\"Maximum vocabulary size: %i. Dictionary size: %i -> %i (removed %i words).\"\n",
        "                    % (max_vocab, init_size, len(self), init_size - len(self)))\n",
        "\n",
        "    @staticmethod\n",
        "    def read_vocab(vocab_path):\n",
        "        \"\"\"\n",
        "        Create a dictionary from a vocabulary file.\n",
        "        \"\"\"\n",
        "        skipped = 0\n",
        "        assert os.path.isfile(vocab_path), vocab_path\n",
        "        word2id = {BOS_WORD: 0, EOS_WORD: 1, PAD_WORD: 2, UNK_WORD: 3}\n",
        "        for i in range(SPECIAL_WORDS):\n",
        "            word2id[SPECIAL_WORD % i] = 4 + i\n",
        "        f = open(vocab_path, 'r', encoding='utf-8')\n",
        "        for i, line in enumerate(f):\n",
        "            if '\\u2028' in line:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            fields = line.rstrip().split()\n",
        "            if len(fields) <= 0 or len(fields) > 2:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            if fields[0] in word2id:\n",
        "                skipped += 1\n",
        "                print('%s already in vocab' % fields[0])\n",
        "                continue\n",
        "            word2id[fields[0]] = 4 + SPECIAL_WORDS + i - skipped  # shift because of extra words\n",
        "        f.close()\n",
        "        id2word = {v: k for k, v in word2id.items()}\n",
        "        dico = Dictionary(id2word, word2id)\n",
        "        logger.info(\"Read %i words from the vocabulary file.\" % len(dico))\n",
        "        if skipped > 0:\n",
        "            logger.warning(\"Skipped %i empty lines!\" % skipped)\n",
        "        return dico\n",
        "\n",
        "    @staticmethod\n",
        "    def index_data(path, bin_path, dico):\n",
        "        \"\"\"\n",
        "        Index sentences with a dictionary.\n",
        "        \"\"\"\n",
        "        if bin_path is not None and os.path.isfile(bin_path):\n",
        "            print(\"Loading dataOld from %s ...\" % bin_path)\n",
        "            data = torch.load(bin_path)\n",
        "            assert dico == data['dico']\n",
        "            return data\n",
        "\n",
        "        positions = []\n",
        "        sentences = []\n",
        "        unk_words = {}\n",
        "\n",
        "        # index sentences\n",
        "        f = open(path, 'r', encoding='utf-8')\n",
        "        for i, line in enumerate(f):\n",
        "            if i % 1000000 == 0 and i > 0:\n",
        "                print(i)\n",
        "            s = line.rstrip().split()\n",
        "            # skip empty sentences\n",
        "            if len(s) == 0:\n",
        "                print(\"Empty sentence in line %i.\" % i)\n",
        "            # index sentence words\n",
        "            count_unk = 0\n",
        "            indexed = []\n",
        "            for w in s:\n",
        "                word_id = dico.index(w, no_unk=False)\n",
        "                # if we find a special word which is not an unknown word, skip the sentence\n",
        "                if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:\n",
        "                    logger.warning('Found unexpected special word \"%s\" (%i)!!' % (w, word_id))\n",
        "                    continue\n",
        "                assert word_id >= 0\n",
        "                indexed.append(word_id)\n",
        "                if word_id == dico.unk_index:\n",
        "                    unk_words[w] = unk_words.get(w, 0) + 1\n",
        "                    count_unk += 1\n",
        "            # add sentence\n",
        "            positions.append([len(sentences), len(sentences) + len(indexed)])\n",
        "            sentences.extend(indexed)\n",
        "            sentences.append(1)  # EOS index\n",
        "        f.close()\n",
        "\n",
        "        # tensorize dataOld\n",
        "        positions = np.int64(positions)\n",
        "        if len(dico) < 1 << 16:\n",
        "            sentences = np.uint16(sentences)\n",
        "        elif len(dico) < 1 << 31:\n",
        "            sentences = np.int32(sentences)\n",
        "        else:\n",
        "            raise Exception(\"Dictionary is too big.\")\n",
        "\n",
        "        assert sentences.min() >= 0\n",
        "        data = {\n",
        "            'dico': dico,\n",
        "            'positions': positions,\n",
        "            'sentences': sentences,\n",
        "            'unk_words': unk_words,\n",
        "        }\n",
        "        if bin_path is not None:\n",
        "            print(\"Saving the dataOld to %s ...\" % bin_path)\n",
        "            torch.save(data, bin_path, pickle_protocol=4)\n",
        "\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def index_table(table_path, table_label_path, table_dico, bin_path):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        if bin_path is not None and os.path.isfile(bin_path):\n",
        "            print(\"Loading dataOld from %s ...\" % bin_path)\n",
        "            data = torch.load(bin_path)\n",
        "            assert table_dico == data['dico']\n",
        "            print(\"%s checked. Nothing is done.\" % bin_path)\n",
        "            return data\n",
        "\n",
        "        positions = []\n",
        "        table_seq_entity = []\n",
        "        table_seq_type = []\n",
        "        table_seq_value = []\n",
        "        table_seq_feat = []\n",
        "        table_seq_label = []\n",
        "\n",
        "        # index table_contents\n",
        "        table_inf = open(table_path, 'r', encoding='utf-8')\n",
        "        table_label_inf = open(table_label_path, 'r', encoding='utf-8')\n",
        "\n",
        "        for i, (line, label_line) in enumerate(zip(table_inf, table_label_inf)):\n",
        "            table_items = line.strip().split()\n",
        "            table_label = label_line.strip().split()\n",
        "            assert len(table_items) == len(table_label)\n",
        "            # skip empty table_contents\n",
        "            if len(table_items) == 0:\n",
        "                print(\"Empty sentence in line %i.\" % i)\n",
        "                continue\n",
        "\n",
        "            # entity, type, value, feat\n",
        "            table_entity_indexed = []\n",
        "            table_type_indexed = []\n",
        "            table_value_indexed = []\n",
        "            table_feat_indexed = []\n",
        "\n",
        "            table_label_indexed = []\n",
        "\n",
        "            for item, label in zip(table_items, table_label):\n",
        "                fields = item.split('|')\n",
        "                assert len(fields) == 4\n",
        "                entity_id = table_dico.index(fields[0], no_unk=False)\n",
        "                type_id = table_dico.index(fields[1], no_unk=False)\n",
        "                value_id = table_dico.index(fields[2], no_unk=False)\n",
        "                feat_id = table_dico.index(fields[3], no_unk=False)\n",
        "\n",
        "                label_id = int(label)\n",
        "\n",
        "                table_entity_indexed.append(entity_id)\n",
        "                table_type_indexed.append(type_id)\n",
        "                table_value_indexed.append(value_id)\n",
        "                table_feat_indexed.append(feat_id)\n",
        "\n",
        "                table_label_indexed.append(label_id)\n",
        "\n",
        "            # add sentence\n",
        "            positions.append([len(table_seq_entity), len(table_seq_entity) + len(table_entity_indexed)])\n",
        "            table_seq_entity.extend(table_entity_indexed)\n",
        "            table_seq_entity.append(1)  # EOS index\n",
        "\n",
        "            table_seq_type.extend(table_type_indexed)\n",
        "            table_seq_type.append(0)  # empty feat\n",
        "\n",
        "            table_seq_value.extend(table_value_indexed)\n",
        "            table_seq_value.append(0)\n",
        "\n",
        "            table_seq_feat.extend(table_feat_indexed)\n",
        "            table_seq_feat.append(0)\n",
        "\n",
        "            table_seq_label.extend(table_label_indexed)\n",
        "            table_seq_label.append(0)\n",
        "\n",
        "        table_inf.close()\n",
        "        table_label_inf.close()\n",
        "\n",
        "        # tensorize dataOld\n",
        "        positions = np.int64(positions)\n",
        "        if len(table_dico) < 1 << 16:\n",
        "            table_seq_entity = np.uint16(table_seq_entity)\n",
        "            table_seq_type = np.uint16(table_seq_type)\n",
        "            table_seq_value = np.uint16(table_seq_value)\n",
        "            table_seq_feat = np.uint16(table_seq_feat)\n",
        "        elif len(table_dico) < 1 << 31:\n",
        "            table_seq_entity = np.int32(table_seq_entity)\n",
        "            table_seq_type = np.uint32(table_seq_type)\n",
        "            table_seq_value = np.uint32(table_seq_value)\n",
        "            table_seq_feat = np.uint32(table_seq_feat)\n",
        "        else:\n",
        "            raise Exception(\"Dictionary is too big.\")\n",
        "\n",
        "        table_seq_label = np.uint8(table_seq_label)\n",
        "\n",
        "        data = {\n",
        "            'dico': table_dico,\n",
        "            'positions': positions,\n",
        "            'table_entities': table_seq_entity,\n",
        "            'table_types': table_seq_type,\n",
        "            'table_values': table_seq_value,\n",
        "            'table_feats': table_seq_feat,\n",
        "            'table_labels': table_seq_label,\n",
        "        }\n",
        "        if bin_path is not None:\n",
        "            print(\"Saving the dataOld to %s ...\" % bin_path)\n",
        "            torch.save(data, bin_path, pickle_protocol=4)\n",
        "\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def index_summary(summary_path, summary_label_path, dico, bin_path, max_len=600):\n",
        "        \"\"\"\n",
        "        Index summaries with a dictionary.\n",
        "        \"\"\"\n",
        "        if bin_path is not None and os.path.isfile(bin_path):\n",
        "            print(\"Loading dataOld from %s ...\" % bin_path)\n",
        "            data = torch.load(bin_path)\n",
        "            assert dico == data['dico']\n",
        "            print(\"%s checked. Nothing is done.\" % bin_path)\n",
        "            return data\n",
        "\n",
        "        positions = []\n",
        "        summaries = []\n",
        "        summary_labels = []\n",
        "\n",
        "        # index summaries\n",
        "        summary_inf = open(summary_path, 'r', encoding='utf-8')\n",
        "        summary_label_inf = open(summary_label_path, 'r', encoding='utf-8')\n",
        "\n",
        "        for i, (summary_line, label_line) in enumerate(zip(summary_inf, summary_label_inf)):\n",
        "            if i % 1000000 == 0 and i > 0:\n",
        "                print(i)\n",
        "            summary_tokens = summary_line.rstrip().split()\n",
        "            summary_token_labels = label_line.rstrip().split()\n",
        "            assert len(summary_tokens) == len(summary_token_labels)\n",
        "            # skip empty summaries\n",
        "            if len(summary_tokens) == 0:\n",
        "                print(\"Empty sentence in line %i.\" % i)\n",
        "                continue\n",
        "            # index sentence words\n",
        "            summary_indexed = []\n",
        "            summary_label_indexed = []\n",
        "            for token, label in zip(summary_tokens, summary_token_labels):\n",
        "                word_id = dico.index(token, no_unk=False)\n",
        "                label_id = int(label)\n",
        "                # if we find a special word which is not an unknown word, skip the sentence\n",
        "                if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:\n",
        "                    logger.warning('Found unexpected special word \"%s\" (%i)!!' % (token, word_id))\n",
        "                    continue\n",
        "                assert word_id >= 0\n",
        "\n",
        "                summary_indexed.append(word_id)\n",
        "                summary_label_indexed.append(label_id)\n",
        "            if len(summary_indexed) > max_len:\n",
        "                summary_indexed = summary_indexed[:max_len]\n",
        "                summary_label_indexed = summary_label_indexed[:max_len]\n",
        "\n",
        "            # add sentence\n",
        "            positions.append([len(summaries), len(summaries) + len(summary_indexed)])\n",
        "            summaries.extend(summary_indexed)\n",
        "            summaries.append(1)  # EOS index\n",
        "            summary_labels.extend(summary_label_indexed)\n",
        "            summary_labels.append(0)\n",
        "\n",
        "        summary_inf.close()\n",
        "        summary_label_inf.close()\n",
        "\n",
        "        # tensorize dataOld\n",
        "        positions = np.int64(positions)\n",
        "        if len(dico) < 1 << 16:\n",
        "            summaries = np.uint16(summaries)\n",
        "        elif len(dico) < 1 << 31:\n",
        "            summaries = np.int32(summaries)\n",
        "        else:\n",
        "            raise Exception(\"Dictionary is too big.\")\n",
        "\n",
        "        summary_labels = np.uint8(summary_labels)\n",
        "\n",
        "        assert summaries.min() >= 0\n",
        "        data = {\n",
        "            'dico': dico,\n",
        "            'positions': positions,\n",
        "            'summaries': summaries,\n",
        "            'summary_labels': summary_labels,\n",
        "        }\n",
        "        if bin_path is not None:\n",
        "            print(\"Saving the dataOld to %s ...\" % bin_path)\n",
        "            torch.save(data, bin_path, pickle_protocol=4)\n",
        "\n",
        "        return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2g0_NYLVCH"
      },
      "source": [
        "#### Transformer Deconder & Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVWmRajlLYH5"
      },
      "source": [
        "N_MAX_POSITIONS = 602  # maximum summary sequence length\n",
        "\n",
        "DECODER_ONLY_PARAMS = [\n",
        "    'layer_norm15.%i.weight', 'layer_norm15.%i.bias',\n",
        "    'encoder_attn.%i.q_lin.weight', 'encoder_attn.%i.q_lin.bias',\n",
        "    'encoder_attn.%i.k_lin.weight', 'encoder_attn.%i.k_lin.bias',\n",
        "    'encoder_attn.%i.v_lin.weight', 'encoder_attn.%i.v_lin.bias',\n",
        "    'encoder_attn.%i.out_lin.weight', 'encoder_attn.%i.out_lin.bias'\n",
        "]\n",
        "\n",
        "TRANSFORMER_LAYER_PARAMS = [\n",
        "    'attentions.%i.q_lin.weight', 'attentions.%i.q_lin.bias',\n",
        "    'attentions.%i.k_lin.weight', 'attentions.%i.k_lin.bias',\n",
        "    'attentions.%i.v_lin.weight', 'attentions.%i.v_lin.bias',\n",
        "    'attentions.%i.out_lin.weight', 'attentions.%i.out_lin.bias',\n",
        "    'layer_norm1.%i.weight', 'layer_norm1.%i.bias',\n",
        "    'ffns.%i.lin1.weight', 'ffns.%i.lin1.bias',\n",
        "    'ffns.%i.lin2.weight', 'ffns.%i.lin2.bias',\n",
        "    'layer_norm2.%i.weight', 'layer_norm2.%i.bias'\n",
        "]\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "def smoothed_softmax_cross_entropy_with_logits(logits, labels, smoothing=0.0):\n",
        "    if not smoothing:\n",
        "        loss = F.cross_entropy(logits, labels, reduction='mean')\n",
        "        return loss\n",
        "\n",
        "    vocab_size = logits.size(1)\n",
        "    n = (vocab_size - 1)\n",
        "    p = 1.0 - smoothing\n",
        "    q = smoothing / n\n",
        "\n",
        "    one_hot = torch.randn(1, vocab_size, device=logits.device)\n",
        "    one_hot.fill_(q)\n",
        "    soft_targets = one_hot.repeat(labels.view(-1, 1).size(0), 1)\n",
        "    soft_targets.scatter_(1, labels.view(-1, 1), p)\n",
        "\n",
        "    log_prb = F.log_softmax(logits, dim=1)\n",
        "    loss = -(soft_targets * log_prb).sum(dim=1)\n",
        "    loss = loss.sum() / len(labels)\n",
        "    return loss\n",
        "\n",
        "def Embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
        "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
        "    if padding_idx is not None:\n",
        "        nn.init.constant_(m.weight[padding_idx], 0)\n",
        "    return m\n",
        "\n",
        "\n",
        "def Linear(in_features, out_features, bias=True):\n",
        "    m = nn.Linear(in_features, out_features, bias)\n",
        "    # nn.init.normal_(m.weight, mean=0, std=1)\n",
        "    # nn.init.xavier_uniform_(m.weight)\n",
        "    # nn.init.constant_(m.bias, 0.)\n",
        "    return m\n",
        "\n",
        "\n",
        "def create_sinusoidal_embeddings(n_pos, dim, out):\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
        "        for pos in range(n_pos)\n",
        "    ])\n",
        "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "    out.detach_()\n",
        "    out.requires_grad = False\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    GELU activation\n",
        "    https://arxiv.org/abs/1606.08415\n",
        "    https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/model_pytorch.py#L14\n",
        "    https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/modeling.py\n",
        "    \"\"\"\n",
        "    # return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def get_masks(slen, lengths, causal):\n",
        "    \"\"\"\n",
        "    Generate hidden states mask, and optionally an attention mask.\n",
        "    \"\"\"\n",
        "    assert lengths.max().item() <= slen\n",
        "    bs = lengths.size(0)\n",
        "    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
        "    mask = alen < lengths[:, None]\n",
        "\n",
        "    # attention mask is the same as mask, or triangular inferior attention (causal)\n",
        "    if causal:\n",
        "        attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]\n",
        "    else:\n",
        "        attn_mask = mask\n",
        "\n",
        "    # sanity check\n",
        "    assert mask.size() == (bs, slen)\n",
        "    assert causal is False or attn_mask.size() == (bs, slen, slen)\n",
        "\n",
        "    return mask, attn_mask\n",
        "\n",
        "class BinaryOutputLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    0/1 Classification layer (binary classification).\n",
        "    \"\"\"\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.pad_index = params.pad_index\n",
        "        dim = params.emb_dim\n",
        "        self.proj = Linear(dim, 1, bias=True)\n",
        "        self.proj_act = nn.Sigmoid()\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the loss, and optionally the scores.\n",
        "        \"\"\"\n",
        "        scores = self.proj(x)\n",
        "        scores = self.proj_act(scores)\n",
        "        y = y.view_as(scores).float()\n",
        "        loss = self.criterion(scores, y)\n",
        "        return scores, loss\n",
        "\n",
        "    def get_scores(self, x):\n",
        "        \"\"\"\n",
        "        Compute scores.\n",
        "        \"\"\"\n",
        "        scores = self.proj(x)\n",
        "        scores = self.proj_act(scores)\n",
        "        return scores\n",
        "\n",
        "\n",
        "class PredLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Prediction layer (cross_entropy or adaptive_softmax).\n",
        "    \"\"\"\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.tgt_n_words = params.tgt_n_words\n",
        "        self.pad_index = params.pad_index\n",
        "        dim = params.emb_dim\n",
        "        try:\n",
        "            self.label_smoothing = params.label_smoothing\n",
        "        except:\n",
        "            self.label_smoothing = 0.0\n",
        "\n",
        "        self.proj = Linear(dim, self.tgt_n_words, bias=True)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the loss, and optionally the scores.\n",
        "        \"\"\"\n",
        "        assert (y == self.pad_index).sum().item() == 0\n",
        "        logits = self.proj(x).view(-1, self.tgt_n_words)\n",
        "        loss = smoothed_softmax_cross_entropy_with_logits(logits, y, self.label_smoothing)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def get_scores(self, x):\n",
        "        \"\"\"\n",
        "        Compute scores.\n",
        "        \"\"\"\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    NEW_ID = itertools.count()\n",
        "\n",
        "    def __init__(self, n_heads, dim, dropout):\n",
        "        super().__init__()\n",
        "        self.layer_id = next(MultiHeadAttention.NEW_ID)\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "        assert self.dim % self.n_heads == 0\n",
        "\n",
        "        self.q_lin = Linear(dim, dim)\n",
        "        self.k_lin = Linear(dim, dim)\n",
        "        self.v_lin = Linear(dim, dim)\n",
        "        self.out_lin = Linear(dim, dim)\n",
        "\n",
        "    def forward(self, input, mask, kv=None, cache=None):\n",
        "        \"\"\"\n",
        "        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n",
        "        \"\"\"\n",
        "        # Input is (bs, qlen, dim)\n",
        "        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n",
        "        bs, qlen, dim = input.size()\n",
        "        if kv is None:\n",
        "            klen = qlen if cache is None else cache['slen'] + qlen\n",
        "        else:\n",
        "            klen = kv.size(1)\n",
        "        assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)\n",
        "        n_heads = self.n_heads\n",
        "        dim_per_head = dim // n_heads\n",
        "        mask_reshape = (bs, 1, qlen, klen) if mask.dim() == 3 else (bs, 1, 1, klen)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\"  projection \"\"\"\n",
        "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            \"\"\"  compute context \"\"\"\n",
        "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
        "\n",
        "        q = shape(self.q_lin(input))                                          # (bs, n_heads, qlen, dim_per_head)\n",
        "        if kv is None:\n",
        "            k = shape(self.k_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n",
        "            v = shape(self.v_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n",
        "        elif cache is None or self.layer_id not in cache:\n",
        "            k = v = kv\n",
        "            k = shape(self.k_lin(k))                                          # (bs, n_heads, qlen, dim_per_head)\n",
        "            v = shape(self.v_lin(v))                                          # (bs, n_heads, qlen, dim_per_head)\n",
        "\n",
        "        if cache is not None:\n",
        "            if self.layer_id in cache:\n",
        "                if kv is None:\n",
        "                    k_, v_ = cache[self.layer_id]\n",
        "                    k = torch.cat([k_, k], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
        "                    v = torch.cat([v_, v], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
        "                else:\n",
        "                    k, v = cache[self.layer_id]\n",
        "            cache[self.layer_id] = (k, v)\n",
        "\n",
        "        q = q / math.sqrt(dim_per_head)                                       # (bs, n_heads, qlen, dim_per_head)\n",
        "        scores = torch.matmul(q, k.transpose(2, 3))                           # (bs, n_heads, qlen, klen)\n",
        "        mask = (mask == 0).view(mask_reshape).expand_as(scores)               # (bs, n_heads, qlen, klen)\n",
        "        scores.masked_fill_(mask, -float('inf'))                              # (bs, n_heads, qlen, klen)\n",
        "\n",
        "        weights = F.softmax(scores.float(), dim=-1).type_as(scores)           # (bs, n_heads, qlen, klen)\n",
        "        weights = F.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n",
        "        context = torch.matmul(weights, v)                                    # (bs, n_heads, qlen, dim_per_head)\n",
        "        context = unshape(context)                                            # (bs, qlen, dim)\n",
        "\n",
        "        return self.out_lin(context)\n",
        "\n",
        "\n",
        "class TransformerFFN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, dim_hidden, out_dim, dropout, gelu_activation):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.lin1 = Linear(in_dim, dim_hidden)\n",
        "        self.lin2 = Linear(dim_hidden, out_dim)\n",
        "        self.act = gelu if gelu_activation else F.relu\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.lin1(input)\n",
        "        x = self.act(x)\n",
        "        x = self.lin2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, params, dico, with_output):\n",
        "        \"\"\"\n",
        "        Transformer model (encoder or decoder).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.with_output = with_output\n",
        "        self.with_positional_emb = params.encoder_positional_emb\n",
        "\n",
        "        self.n_words = params.src_n_words\n",
        "\n",
        "        self.eos_index = params.eos_index\n",
        "        self.pad_index = params.pad_index\n",
        "        self.dico = dico\n",
        "        assert len(self.dico) == self.n_words\n",
        "\n",
        "        # model parameters\n",
        "        self.emb_dim = params.emb_dim // 4  # 128 by default\n",
        "        self.dim = params.emb_dim       # 512 by default\n",
        "        self.hidden_dim = self.dim * 4  # 2048 by default\n",
        "        self.n_heads = params.n_heads   # 8 by default\n",
        "        self.n_layers = params.enc_n_layers\n",
        "        self.dropout = params.dropout\n",
        "        self.attention_dropout = params.attention_dropout\n",
        "        assert self.dim % self.n_heads == 0, 'transformer dim must be a multiple of n_heads'\n",
        "\n",
        "        # embeddings\n",
        "        # TODO remove the hardcode number\n",
        "        if params.encoder_positional_emb:\n",
        "            self.position_embeddings = Embedding(800, self.dim)\n",
        "            if params.sinusoidal_embeddings:\n",
        "                create_sinusoidal_embeddings(800, self.dim, out=self.position_embeddings.weight)\n",
        "        self.embeddings = Embedding(self.n_words, self.emb_dim, padding_idx=self.pad_index)\n",
        "        self.layer_norm_emb = nn.LayerNorm(self.dim, eps=1e-12)\n",
        "\n",
        "        # transformer layers\n",
        "        self.attentions = nn.ModuleList()\n",
        "        self.layer_norm1 = nn.ModuleList()\n",
        "        self.ffns = nn.ModuleList()\n",
        "        self.layer_norm2 = nn.ModuleList()\n",
        "\n",
        "        for _ in range(self.n_layers):\n",
        "            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n",
        "            self.layer_norm1.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "            self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, dropout=self.dropout, gelu_activation=params.gelu_activation))\n",
        "            self.layer_norm2.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "\n",
        "        # output layer\n",
        "        self.pred_layer = BinaryOutputLayer(params)\n",
        "\n",
        "    def forward(self, mode, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward function with different forward modes.\n",
        "        ### Small hack to handle PyTorch distributed.\n",
        "        \"\"\"\n",
        "        if mode == 'fwd':\n",
        "            return self.fwd(**kwargs)\n",
        "        elif mode == 'predict':\n",
        "            return self.predict(**kwargs)\n",
        "        elif mode == 'score':\n",
        "            return self.get_scores(**kwargs)\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode: %s\" % mode)\n",
        "\n",
        "    def fwd(self, x1, x2, x3, x4, lengths):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            `x` LongTensor(slen, bs), containing word indices\n",
        "            `lengths` LongTensor(bs), containing the length of each sentence\n",
        "            `causal` Boolean, if True, the attention is only done over previous hidden states\n",
        "        \"\"\"\n",
        "        # lengths = (x != self.pad_index).float().sum(dim=1)\n",
        "        # mask = x != self.pad_index\n",
        "\n",
        "        # check inputs\n",
        "        slen, bs = x1.size()\n",
        "        assert lengths.size(0) == bs\n",
        "        assert lengths.max().item() <= slen\n",
        "        x1 = self.embeddings(x1.transpose(0, 1))\n",
        "        x2 = self.embeddings(x2.transpose(0, 1))\n",
        "        x3 = self.embeddings(x3.transpose(0, 1))\n",
        "        x4 = self.embeddings(x4.transpose(0, 1))\n",
        "\n",
        "        # generate masks\n",
        "        mask, attn_mask = get_masks(slen, lengths, causal=False)\n",
        "\n",
        "        positions = x1.new(slen).long()\n",
        "        positions = torch.arange(slen, out=positions).unsqueeze(0)\n",
        "\n",
        "        # embeddings\n",
        "        tensor = torch.cat((x1, x2, x3, x4), dim=-1)\n",
        "        if self.with_positional_emb:\n",
        "            tensor = tensor + self.position_embeddings(positions).expand_as(tensor)\n",
        "        #tensor = x1 + x2 + x3 + x4\n",
        "        tensor = self.layer_norm_emb(tensor)\n",
        "        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n",
        "        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            # self attention\n",
        "            attn = self.attentions[i](tensor, attn_mask)\n",
        "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "            tensor = tensor + attn\n",
        "            tensor = self.layer_norm1[i](tensor)\n",
        "\n",
        "            # FFN\n",
        "            tensor = tensor + self.ffns[i](tensor)\n",
        "            tensor = self.layer_norm2[i](tensor)\n",
        "            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # move back sequence length to dimension 0\n",
        "        tensor = tensor.transpose(0, 1)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def predict(self, tensor, pred_mask, y):\n",
        "        \"\"\"\n",
        "        Given the last hidden state, compute word scores and/or the loss.\n",
        "            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n",
        "                we need to predict a word\n",
        "            `y` is a LongTensor of shape (pred_mask.sum(),)\n",
        "        \"\"\"\n",
        "        masked_tensor = tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim)\n",
        "        scores, loss = self.pred_layer(masked_tensor, y)\n",
        "        return scores, loss\n",
        "\n",
        "    def get_scores(self, tensor):\n",
        "        scores = self.pred_layer.get_scores(tensor)\n",
        "        return scores\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, params, dico, with_output):\n",
        "        \"\"\"\n",
        "        Transformer model (encoder or decoder).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder / decoder, output layer\n",
        "        self.with_output = with_output\n",
        "\n",
        "        # dictionary / languages\n",
        "        self.n_words = params.tgt_n_words\n",
        "            \n",
        "        self.eos_index = params.eos_index\n",
        "        self.pad_index = params.pad_index\n",
        "        self.dico = dico\n",
        "        print(self.n_words)\n",
        "        print(len(self.dico))\n",
        "        assert len(self.dico) == (self.n_words)\n",
        "\n",
        "        # model parameters\n",
        "        self.dim = params.emb_dim       # 512 by default\n",
        "        self.hidden_dim = self.dim * 4  # 2048 by default\n",
        "        self.n_heads = params.n_heads   # 8 by default\n",
        "        self.n_layers = params.dec_n_layers\n",
        "        self.dropout = params.dropout\n",
        "        self.attention_dropout = params.attention_dropout\n",
        "        assert self.dim % self.n_heads == 0, 'transformer dim must be a multiple of n_heads'\n",
        "\n",
        "        # embeddings\n",
        "        self.position_embeddings = Embedding(N_MAX_POSITIONS, self.dim)\n",
        "        if params.sinusoidal_embeddings:\n",
        "            create_sinusoidal_embeddings(N_MAX_POSITIONS, self.dim, out=self.position_embeddings.weight)\n",
        "        self.embeddings = Embedding(self.n_words, self.dim, padding_idx=self.pad_index)\n",
        "        self.layer_norm_emb = nn.LayerNorm(self.dim, eps=1e-12)\n",
        "\n",
        "        # transformer layers\n",
        "        self.attentions = nn.ModuleList()\n",
        "        self.layer_norm1 = nn.ModuleList()\n",
        "        self.ffns = nn.ModuleList()\n",
        "        self.layer_norm2 = nn.ModuleList()\n",
        "        self.layer_norm15 = nn.ModuleList()\n",
        "        self.encoder_attn = nn.ModuleList()\n",
        "\n",
        "        for _ in range(self.n_layers):\n",
        "            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n",
        "            self.layer_norm1.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "            self.layer_norm15.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "            self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n",
        "            self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, dropout=self.dropout, gelu_activation=params.gelu_activation))\n",
        "            self.layer_norm2.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "\n",
        "        if self.with_output:\n",
        "            self.pred_layer = PredLayer(params)\n",
        "            if params.share_inout_emb:\n",
        "                self.pred_layer.proj.weight = self.embeddings.weight\n",
        "\n",
        "    def forward(self, mode, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward function with different forward modes.\n",
        "        ### Small hack to handle PyTorch distributed.\n",
        "        \"\"\"\n",
        "        if mode == 'fwd':\n",
        "            return self.fwd(**kwargs)\n",
        "        elif mode == 'predict':\n",
        "            return self.predict(**kwargs)\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode: %s\" % mode)\n",
        "\n",
        "    def fwd(self, x, lengths, causal, src_enc=None, src_len=None, positions=None, cache=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            `x` LongTensor(slen, bs), containing word indices\n",
        "            `lengths` LongTensor(bs), containing the length of each sentence\n",
        "            `causal` Boolean, if True, the attention is only done over previous hidden states\n",
        "            `positions` LongTensor(slen, bs), containing word positions\n",
        "        \"\"\"\n",
        "        # lengths = (x != self.pad_index).float().sum(dim=1)\n",
        "        # mask = x != self.pad_index\n",
        "\n",
        "        # check inputs\n",
        "        slen, bs = x.size()\n",
        "        assert lengths.size(0) == bs\n",
        "        assert lengths.max().item() <= slen\n",
        "        x = x.transpose(0, 1)  # batch size as dimension 0\n",
        "        assert (src_enc is None) == (src_len is None)\n",
        "        if src_enc is not None:\n",
        "            assert src_enc.size(0) == bs, \"{}!={}\".format(src_enc.size(0), bs)\n",
        "\n",
        "        # generate masks\n",
        "        mask, attn_mask = get_masks(slen, lengths, causal)\n",
        "        if src_enc is not None:\n",
        "            src_mask = torch.arange(src_enc.size(1), dtype=torch.long, device=lengths.device) < src_len[:, None]\n",
        "\n",
        "        # positions\n",
        "        if positions is None:\n",
        "            positions = x.new(slen).long()\n",
        "            positions = torch.arange(slen, out=positions).unsqueeze(0)\n",
        "        else:\n",
        "            assert positions.size() == (slen, bs), positions.size()\n",
        "            positions = positions.transpose(0, 1)\n",
        "\n",
        "        # do not recompute cached elements\n",
        "        if cache is not None:\n",
        "            _slen = slen - cache['slen']\n",
        "            x = x[:, -_slen:]\n",
        "            positions = positions[:, -_slen:]\n",
        "            mask = mask[:, -_slen:]\n",
        "            attn_mask = attn_mask[:, -_slen:]\n",
        "\n",
        "        # embeddings\n",
        "        tensor = self.embeddings(x)\n",
        "        tensor = tensor + self.position_embeddings(positions).expand_as(tensor)\n",
        "        tensor = self.layer_norm_emb(tensor)\n",
        "        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n",
        "        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            # self attention\n",
        "            attn = self.attentions[i](tensor, attn_mask, cache=cache)\n",
        "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "            tensor = tensor + attn\n",
        "            tensor = self.layer_norm1[i](tensor)\n",
        "\n",
        "            # encoder attention (for decoder only)\n",
        "            if src_enc is not None:\n",
        "                attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)\n",
        "                attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "                tensor = tensor + attn\n",
        "                tensor = self.layer_norm15[i](tensor)\n",
        "\n",
        "            # FFN\n",
        "            tensor = tensor + self.ffns[i](tensor)\n",
        "            tensor = self.layer_norm2[i](tensor)\n",
        "            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # update cache length\n",
        "        if cache is not None:\n",
        "            cache['slen'] += tensor.size(1)\n",
        "\n",
        "        # move back sequence length to dimension 0\n",
        "        tensor = tensor.transpose(0, 1)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def predict(self, tensor, pred_mask, y):\n",
        "        \"\"\"\n",
        "        Given the last hidden state, compute word scores and/or the loss.\n",
        "            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n",
        "                we need to predict a word\n",
        "            `y` is a LongTensor of shape (pred_mask.sum(),)\n",
        "        \"\"\"\n",
        "        masked_tensor = tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim)\n",
        "        scores, loss = self.pred_layer(masked_tensor, y)\n",
        "        return scores, loss\n",
        "\n",
        "    def generate(self, src_enc, src_len, max_len=200, sample_temperature=None, vocab_mask=None):\n",
        "        \"\"\"\n",
        "        Decode a sentence given initial start.\n",
        "        `x`:\n",
        "            - LongTensor(bs, slen)\n",
        "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
        "                <EOS> W1 W2 W3   W4  <EOS>\n",
        "        `lengths`:\n",
        "            - LongTensor(bs) [5, 6]\n",
        "        `positions`:\n",
        "            - False, for regular \"arange\" positions (LM)\n",
        "            - True, to reset positions from the new generation (MT)\n",
        "        \"\"\"\n",
        "\n",
        "        # input batch\n",
        "        bs = len(src_len)\n",
        "        assert src_enc.size(0) == bs\n",
        "\n",
        "        # generated sentences\n",
        "        generated = src_len.new(max_len, bs)  # upcoming output\n",
        "        generated.fill_(self.pad_index)       # fill upcoming ouput with <PAD>\n",
        "        generated[0].fill_(self.eos_index)    # we use <EOS> for <BOS> everywhere\n",
        "\n",
        "        # positions\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand(max_len, bs)\n",
        "\n",
        "        # current position / max lengths / length of generated sentences / unfinished sentences\n",
        "        cur_len = 1\n",
        "        gen_len = src_len.clone().fill_(1)\n",
        "        unfinished_sents = src_len.clone().fill_(1)\n",
        "\n",
        "        # cache compute states\n",
        "        cache = {'slen': 0}\n",
        "\n",
        "        while cur_len < max_len:\n",
        "\n",
        "            # compute word scores\n",
        "            tensor = self.forward(\n",
        "                'fwd',\n",
        "                x=generated[:cur_len],\n",
        "                lengths=gen_len,\n",
        "                positions=positions[:cur_len],\n",
        "                causal=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                cache=cache\n",
        "            )\n",
        "            assert tensor.size() == (1, bs, self.dim)\n",
        "            tensor = tensor.data[-1, :, :]               # (bs, dim)\n",
        "            scores = self.pred_layer.get_scores(tensor)  # (bs, n_words)\n",
        "\n",
        "\n",
        "            # select next words: sample or greedy\n",
        "            if sample_temperature is None:\n",
        "                next_words = torch.topk(scores, 1)[1].squeeze(1)\n",
        "            else:\n",
        "                next_words = torch.multinomial(F.softmax(scores / sample_temperature, dim=1), 1).squeeze(1)\n",
        "            assert next_words.size() == (bs,)\n",
        "\n",
        "            # update generations / lengths / finished sentences / current length\n",
        "            generated[cur_len] = next_words * unfinished_sents + self.pad_index * (1 - unfinished_sents)\n",
        "            gen_len.add_(unfinished_sents)\n",
        "            unfinished_sents.mul_(next_words.ne(self.eos_index).long())\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
        "            if unfinished_sents.max() == 0:\n",
        "                break\n",
        "\n",
        "        # add <EOS> to unfinished sentences\n",
        "        if cur_len == max_len:\n",
        "            generated[-1].masked_fill_(unfinished_sents.byte(), self.eos_index)\n",
        "\n",
        "        # sanity check\n",
        "        assert (generated == self.eos_index).sum() == 2 * bs\n",
        "\n",
        "        return generated[:cur_len], gen_len\n",
        "\n",
        "    def generate_beam(self, src_enc, src_len, beam_size, length_penalty, early_stopping, max_len=200):\n",
        "        \"\"\"\n",
        "        Decode a sentence given initial start.\n",
        "        `x`:\n",
        "            - LongTensor(bs, slen)\n",
        "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
        "                <EOS> W1 W2 W3   W4  <EOS>\n",
        "        `lengths`:\n",
        "            - LongTensor(bs) [5, 6]\n",
        "        `positions`:\n",
        "            - False, for regular \"arange\" positions (LM)\n",
        "            - True, to reset positions from the new generation (MT)\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        assert src_enc.size(0) == src_len.size(0)\n",
        "        assert beam_size >= 1\n",
        "\n",
        "        # batch size / number of words\n",
        "        bs = len(src_len)\n",
        "        n_words = self.n_words\n",
        "\n",
        "        # expand to beam size the source latent representations / source lengths\n",
        "        src_enc = src_enc.unsqueeze(1).expand((bs, beam_size) + src_enc.shape[1:]).contiguous().view((bs * beam_size,) + src_enc.shape[1:])\n",
        "        src_len = src_len.unsqueeze(1).expand(bs, beam_size).contiguous().view(-1)\n",
        "\n",
        "        # generated sentences (batch with beam current hypotheses)\n",
        "        generated = src_len.new(max_len, bs * beam_size)  # upcoming output\n",
        "        generated.fill_(self.pad_index)                   # fill upcoming ouput with <PAD>\n",
        "        generated[0].fill_(self.eos_index)                # we use <EOS> for <BOS> everywhere\n",
        "\n",
        "        # generated hypotheses\n",
        "        generated_hyps = [BeamHypotheses(beam_size, max_len, length_penalty, early_stopping) for _ in range(bs)]\n",
        "\n",
        "        # positions\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand_as(generated)\n",
        "\n",
        "        # scores for each sentence in the beam\n",
        "        beam_scores = src_enc.new(bs, beam_size).fill_(0)\n",
        "        beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view(-1)\n",
        "\n",
        "        # current position\n",
        "        cur_len = 1\n",
        "\n",
        "        # cache compute states\n",
        "        cache = {'slen': 0}\n",
        "\n",
        "        # done sentences\n",
        "        done = [False for _ in range(bs)]\n",
        "\n",
        "        while cur_len < max_len:\n",
        "\n",
        "            # compute word scores\n",
        "            tensor = self.forward(\n",
        "                'fwd',\n",
        "                x=generated[:cur_len],\n",
        "                lengths=src_len.new(bs * beam_size).fill_(cur_len),\n",
        "                positions=positions[:cur_len],\n",
        "                causal=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                cache=cache\n",
        "            )\n",
        "            assert tensor.size() == (1, bs * beam_size, self.dim)\n",
        "            tensor = tensor.data[-1, :, :]               # (bs * beam_size, dim)\n",
        "            #print(tensor)\n",
        "            scores = self.pred_layer.get_scores(tensor)  # (bs * beam_size, n_words)\n",
        "            scores = F.log_softmax(scores, dim=-1)       # (bs * beam_size, n_words)\n",
        "            assert scores.size() == (bs * beam_size, n_words)\n",
        "\n",
        "            # select next words with scores\n",
        "            #scores of n top sentences where n = beam_size\n",
        "            _scores = scores + beam_scores[:, None].expand_as(scores)  # (bs * beam_size, n_words)\n",
        "            #print(f'1 {_scores}')\n",
        "            #top score?\n",
        "            _scores = _scores.view(bs, beam_size * n_words)            # (bs, beam_size * n_words)\n",
        "            #print(f'2 {_scores}')\n",
        "            next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)\n",
        "            #test = []\n",
        "            #for idx, value in zip(next_words[0], next_scores[0]):\n",
        "                # get beam and word IDs\n",
        "                #beam_id = idx // n_words\n",
        "                #word_id = idx % n_words\n",
        "                #test.append(self.dico[word_id.item()])\n",
        "                #print(next_score, next_word)\n",
        "                #if next_word in removedDict:\n",
        "                    #print(next_word)\n",
        "            #print(f'next words: {test}')\n",
        "            assert next_scores.size() == next_words.size() == (bs, 2 * beam_size)\n",
        "\n",
        "            # next batch beam content\n",
        "            # list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)\n",
        "            next_batch_beam = []\n",
        "\n",
        "            # for each sentence\n",
        "            for sent_id in range(bs):\n",
        "\n",
        "                # if we are done with this sentence\n",
        "                done[sent_id] = done[sent_id] or generated_hyps[sent_id].is_done(next_scores[sent_id].max().item())\n",
        "                if done[sent_id]:\n",
        "                    next_batch_beam.extend([(0, self.pad_index, 0)] * beam_size)  # pad the batch\n",
        "                    continue\n",
        "\n",
        "                # next sentence beam content\n",
        "                next_sent_beam = []\n",
        "\n",
        "                # next words for this sentence\n",
        "                for idx, value in zip(next_words[sent_id], next_scores[sent_id]):\n",
        "\n",
        "                    # get beam and word IDs\n",
        "                    beam_id = idx // n_words\n",
        "                    word_id = idx % n_words\n",
        "                    #print(, value)\n",
        "                    #print('loop')\n",
        "\n",
        "                    # end of sentence, or next word\n",
        "                    if word_id == self.eos_index or cur_len + 1 == max_len:\n",
        "                        generated_hyps[sent_id].add(generated[:cur_len, sent_id * beam_size + beam_id].clone(), value.item())\n",
        "                        #print(f'hyp: {generated_hyps}')\n",
        "                    else:\n",
        "                        next_sent_beam.append((value, word_id, sent_id * beam_size + beam_id))\n",
        "                        #if word_id.item() in self.dico.id2word.keys():\n",
        "                            #if 'template' in self.dico.id2word[word_id.item()]:\n",
        "                                #if word_id.item() not in [ids[1] for ids in next_sent_beam]:\n",
        "                                    #print(self.dico.id2word[word_id.item()])\n",
        "                                    #next_sent_beam.append((value, word_id, sent_id * beam_size + beam_id))\n",
        "                                #else:\n",
        "                                #    print(f'{self.dico.id2word[word_id.item()]} : already in : {[self.dico.id2word[ids[1].item()] for ids in next_sent_beam]}')\n",
        "                            #else:\n",
        "\n",
        "                        #else:\n",
        "                            #print(f'bad {word_id.item()}')\n",
        "\n",
        "                    # the beam for next step is full\n",
        "                    if len(next_sent_beam) == beam_size:\n",
        "                        break\n",
        "                \"\"\"\n",
        "                for i in range(len(next_sent_beam)):\n",
        "                    #print(self.dico[word_id.item()])\n",
        "                    if self.dico[word_id.item()] in removedDict:\n",
        "                        print(next_sent_beam)\n",
        "                        #if predicted template is not valid, pop the tuple and append it to the end\n",
        "                        badBeam = next_sent_beam[i].pop()\n",
        "                        next_sent_beam.append(badBeam)\n",
        "                        print(next_sent_beam)\n",
        "                        \"\"\"\n",
        "                # update next beam content\n",
        "                assert len(next_sent_beam) == 0 if cur_len + 1 == max_len else beam_size\n",
        "                if len(next_sent_beam) == 0:\n",
        "                    next_sent_beam = [(0, self.pad_index, 0)] * beam_size  # pad the batch\n",
        "                next_batch_beam.extend(next_sent_beam)\n",
        "                assert len(next_batch_beam) == beam_size * (sent_id + 1)\n",
        "\n",
        "            # sanity check / prepare next batch\n",
        "            assert len(next_batch_beam) == bs * beam_size\n",
        "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
        "            beam_words = generated.new([x[1] for x in next_batch_beam])\n",
        "            beam_idx = src_len.new([x[2] for x in next_batch_beam])\n",
        "\n",
        "            #decoded = [self.dico[x.item()] for x in beam_words]\n",
        "            #print(f'decoded: {decoded}')\n",
        "\n",
        "            # re-order batch and internal states\n",
        "            generated = generated[:, beam_idx]\n",
        "            generated[cur_len] = beam_words\n",
        "            #print(generated)\n",
        "            #print(generated[cur_len])\n",
        "            for k in cache.keys():\n",
        "                if k != 'slen':\n",
        "                    cache[k] = (cache[k][0][beam_idx], cache[k][1][beam_idx])\n",
        "\n",
        "            # update current length\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # stop when we are done with each sentence\n",
        "            if all(done):\n",
        "                break\n",
        "\n",
        "        # visualize hypotheses\n",
        "        # print([len(x) for x in generated_hyps], cur_len)\n",
        "        # globals().update( locals() );\n",
        "        # !import code; code.interact(local=vars())\n",
        "        #for ii in range(bs):\n",
        "        #    for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):\n",
        "        #        print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))\n",
        "        #        print(\"\")\n",
        "\n",
        "        # select the best hypotheses\n",
        "        tgt_len = src_len.new(bs)\n",
        "        best = []\n",
        "\n",
        "        for i, hypotheses in enumerate(generated_hyps):\n",
        "            best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
        "            tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
        "            best.append(best_hyp)\n",
        "\n",
        "        # generate target batch\n",
        "        decoded = src_len.new(tgt_len.max().item(), bs).fill_(self.pad_index)\n",
        "        for i, hypo in enumerate(best):\n",
        "            decoded[:tgt_len[i] - 1, i] = hypo\n",
        "            decoded[tgt_len[i] - 1, i] = self.eos_index\n",
        "\n",
        "        # sanity check\n",
        "        assert (decoded == self.eos_index).sum() == 2 * bs\n",
        "\n",
        "        return decoded, tgt_len\n",
        "\n",
        "    def generate_slot(self, x, xlen, y_type, src_enc, src_len, sample_temperature=None):\n",
        "        # input batch\n",
        "        bs = len(src_len)\n",
        "        assert src_enc.size(0) == bs, \"{}!={}\".format(src_enc.size(0), bs)\n",
        "        max_len = xlen.max()\n",
        "\n",
        "        indices = (x==4).nonzero()\n",
        "        pred_indices, batch_indices = torch.split(indices, 1, 1)\n",
        "\n",
        "        # generated sentences\n",
        "        generated = x.clone()\n",
        "\n",
        "        # positions\n",
        "        positions = torch.arange(max_len, dtype=torch.long, device=src_len.device)\n",
        "        positions = positions.unsqueeze(1).expand(max_len, bs)\n",
        "\n",
        "        # current position / max lengths / length of generated sentences / unfinished sentences\n",
        "        cur_idx = 0\n",
        "        max_idx = pred_indices.size(0)\n",
        "\n",
        "        while cur_idx < max_idx:\n",
        "            cur_len = pred_indices[cur_idx].item()\n",
        "            batch_inx = batch_indices[cur_idx].item()\n",
        "\n",
        "            cur_type = y_type[cur_len,batch_inx]\n",
        "            # compute word scores\n",
        "            tensor = self.forward(\n",
        "                'fwd',\n",
        "                x=generated[:cur_len, batch_inx][:,None],\n",
        "                lengths=pred_indices[cur_idx],\n",
        "                positions=positions[:cur_len, batch_inx][:,None],\n",
        "                causal=True,\n",
        "                src_enc=src_enc[batch_inx][None],\n",
        "                src_len=src_len[batch_inx][None],\n",
        "                cache=None\n",
        "            )\n",
        "            #assert tensor.size() == (1, bs, self.dim)\n",
        "            tensor = tensor.data[-1, :, :]               # (bs, dim)\n",
        "            tensor_type_embedding = self.prediction_type_embeddings(cur_type)\n",
        "            tensor = torch.cat((tensor, tensor_type_embedding.unsqueeze(0)), dim=-1)\n",
        "            tensor = self.prediction_type_pooler(tensor)\n",
        "            scores = self.pred_layer.get_scores(tensor)  # (bs, n_words)\n",
        "\n",
        "\n",
        "            # select next words: sample or greedy\n",
        "            if sample_temperature is None:\n",
        "                # take top-2 to avoid generating <eos>\n",
        "                next_words = torch.topk(scores, 2)[1].squeeze()\n",
        "            else:\n",
        "                next_words = torch.multinomial(F.softmax(scores / sample_temperature, dim=1), 1).squeeze(1)\n",
        "            assert next_words.size() == (2,), next_words.size()\n",
        "\n",
        "            # update generations / lengths / finished sentences / current length\n",
        "            # generated[cur_len, batch_inx] = next_words\n",
        "            generated[cur_len, batch_inx] = next_words[0] if next_words[0] != self.eos_index else next_words[1]\n",
        "            cur_idx = cur_idx + 1\n",
        "\n",
        "        # sanity check\n",
        "        assert (generated == self.eos_index).sum() == 2 * bs, (generated == self.eos_index).sum()\n",
        "\n",
        "        return generated\n",
        "\n",
        "    def generate_slot_beam(self, x, xlen, y_type, src_enc, src_len, beam_size=2):\n",
        "        # input batch\n",
        "        bs = len(src_len)\n",
        "        n_words = self.n_words\n",
        "        assert src_enc.size(0) == bs, \"{}!={}\".format(src_enc.size(0), bs)\n",
        "        max_len = xlen.max()\n",
        "\n",
        "        # expand to beam size the source latent representations / source lengths\n",
        "        src_enc = src_enc.unsqueeze(1).expand((bs, beam_size) + src_enc.shape[1:]).contiguous()\n",
        "        src_len = src_len.unsqueeze(1).expand(bs, beam_size).contiguous()\n",
        "        \n",
        "        result = []\n",
        "        for batch_idx in range(bs):\n",
        "            x_i = x[:,batch_idx]\n",
        "            pred_indices = (x_i==4).nonzero()\n",
        "\n",
        "            src_enc_sub = src_enc[batch_idx]\n",
        "            src_len_sub = src_len[batch_idx]\n",
        "\n",
        "            # generated sentences\n",
        "            generated = x_i.clone()\n",
        "            generated = generated.unsqueeze(1).expand(max_len, beam_size)\n",
        "\n",
        "            positions = torch.arange(max_len, dtype=torch.long, device=src_len.device)\n",
        "            positions = positions.unsqueeze(1).expand(max_len, beam_size)\n",
        "\n",
        "            # scores for each sentence in the beam\n",
        "            beam_scores = src_enc.new(1, beam_size).fill_(0)\n",
        "            beam_scores[:, 1:] = -1e9\n",
        "            beam_scores = beam_scores.view(-1)\n",
        "\n",
        "            cur_len = 1\n",
        "            cache = {'slen': 0}\n",
        "\n",
        "            while cur_len < max_len:\n",
        "                cur_type = y_type[cur_len, batch_idx]\n",
        "                # compute word scores\n",
        "                tensor = self.forward(\n",
        "                    'fwd',\n",
        "                    x=generated[:cur_len],\n",
        "                    lengths=src_len.new(beam_size).fill_(cur_len),\n",
        "                    positions=positions[:cur_len],\n",
        "                    causal=True,\n",
        "                    src_enc=src_enc_sub,\n",
        "                    src_len=src_len_sub,\n",
        "                    cache=cache\n",
        "                )\n",
        "                assert tensor.size() == (1, beam_size, self.dim)\n",
        "                tensor = tensor.data[-1, :, :]               # (beam_size, dim)\n",
        "                if cur_type.item() == 0:\n",
        "                    tgt_id = x_i[cur_len].item()\n",
        "                    scores = self.pred_layer.get_scores(tensor)  # (beam_size, n_words)\n",
        "                    scores = scores[:, tgt_id] # get the target token score\n",
        "                    scores = F.log_softmax(scores, dim=-1)       # (beam_size)\n",
        "                    assert scores.size() == (beam_size,)\n",
        "                    \n",
        "                    # here we don't need to sort the beam order\n",
        "                    beam_scores = beam_scores + scores\n",
        "                else:\n",
        "                    tensor_type = self.prediction_type_embeddings(cur_type)\n",
        "                    tensor_type = tensor_type.unsqueeze(0).expand_as(tensor)\n",
        "                    tensor = torch.cat((tensor, tensor_type), dim=-1)\n",
        "                    tensor = self.prediction_type_pooler(tensor)\n",
        "                    scores = self.pred_layer.get_scores(tensor)  # (beam_size, n_words)\n",
        "                    scores = F.log_softmax(scores, dim=-1)       # (beam_size, n_words)\n",
        "                    assert scores.size() == (beam_size, n_words)\n",
        "\n",
        "                    # select next words with scores\n",
        "                    _scores = scores + beam_scores[:, None].expand_as(scores)  # (beam_size, n_words)\n",
        "                    _scores = _scores.view(1, beam_size * n_words)             # (1, beam_size * n_words)\n",
        "\n",
        "                    next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)\n",
        "                    assert next_scores.size() == next_words.size() == (1, 2 * beam_size)\n",
        "\n",
        "                    # next sentence beam content\n",
        "                    next_sent_beam = []\n",
        "\n",
        "                    for idx, value in zip(next_words[0], next_scores[0]):\n",
        "                        beam_id = idx // n_words\n",
        "                        word_id = idx % n_words\n",
        "\n",
        "                        # end of sentence, or next word\n",
        "                        if word_id != self.eos_index:\n",
        "                            next_sent_beam.append((value, word_id, beam_id))\n",
        "\n",
        "                        # the beam for next step is full\n",
        "                        if len(next_sent_beam) == beam_size:\n",
        "                            break\n",
        "\n",
        "                    beam_scores = beam_scores.new([x[0] for x in next_sent_beam])\n",
        "                    beam_words  = generated.new([x[1] for x in next_sent_beam])\n",
        "                    beam_idx = src_len.new([x[2] for x in next_sent_beam])\n",
        "\n",
        "                    generated = generated[:, beam_idx]\n",
        "                    generated[cur_len] = beam_words\n",
        "            \n",
        "                    for k in cache.keys():\n",
        "                        if k != 'slen':\n",
        "                            cache[k] = (cache[k][0][beam_idx], cache[k][1][beam_idx])\n",
        "                cur_len += 1\n",
        "            # sanity check\n",
        "            assert (generated == self.eos_index).sum() == 2 * beam_size, (generated == self.eos_index).sum()\n",
        "            max_score, index = beam_scores.max(0)\n",
        "            result.append(generated[:,index].unsqueeze(1))\n",
        "\n",
        "        result = torch.cat(result, dim=1)\n",
        "        assert result.size() == x.size()\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class BeamHypotheses(object):\n",
        "\n",
        "    def __init__(self, n_hyp, max_len, length_penalty, early_stopping):\n",
        "        \"\"\"\n",
        "        Initialize n-best list of hypotheses.\n",
        "        \"\"\"\n",
        "        self.max_len = max_len - 1  # ignoring <BOS>\n",
        "        self.length_penalty = length_penalty\n",
        "        self.early_stopping = early_stopping\n",
        "        self.n_hyp = n_hyp\n",
        "        self.hyp = []\n",
        "        self.worst_score = 1e9\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of hypotheses in the list.\n",
        "        \"\"\"\n",
        "        return len(self.hyp)\n",
        "\n",
        "    def add(self, hyp, sum_logprobs):\n",
        "        \"\"\"\n",
        "        Add a new hypothesis to the list.\n",
        "        \"\"\"\n",
        "        #print(self.removedDict)\n",
        "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
        "        if len(self) < self.n_hyp or score > self.worst_score:\n",
        "            #if hyp not in self.removedDict:\n",
        "                #print(score, hyp)\n",
        "            self.hyp.append((score, hyp))\n",
        "            if len(self) > self.n_hyp:\n",
        "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
        "                del self.hyp[sorted_scores[0][1]]\n",
        "                self.worst_score = sorted_scores[1][0]\n",
        "            else:\n",
        "                self.worst_score = min(score, self.worst_score)\n",
        "\n",
        "    def is_done(self, best_sum_logprobs):\n",
        "        \"\"\"\n",
        "        If there are enough hypotheses and that none of the hypotheses being generated\n",
        "        can become better than the worst one in the heap, then we are done with this sentence.\n",
        "        \"\"\"\n",
        "        if len(self) < self.n_hyp:\n",
        "            return False\n",
        "        elif self.early_stopping:\n",
        "            return True\n",
        "        else:\n",
        "            return self.worst_score >= best_sum_logprobs / self.max_len ** self.length_penalty"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB1A-1yiEAtx"
      },
      "source": [
        "### Titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA3MeXhOjXTz"
      },
      "source": [
        "def clean_title(title):\n",
        "  cleanedTitle = word_tokenize(title)\n",
        "  # replace (2009 - 2016) with (2009 to 2016)\n",
        "  lastTokens = cleanedTitle[-3:]\n",
        "  if lastTokens[1] == '-' and lastTokens[0].isnumeric() and lastTokens[2].isnumeric():\n",
        "    cleanedTitle[-2] = 'to'\n",
        "  cleanedTitle = ' '.join(cleanedTitle).replace('*', '')\n",
        "  return cleanedTitle"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7dA-aZzED5b"
      },
      "source": [
        "###Preprocessing\n",
        "- Converts data tables into a sequence of records (taken as input by the model): `data/*split*/trainData.txt`\n",
        "- Cleans summary tokens and substitutes any possible tokens with data variables(e.g., 2018 -> templateValue[0][0]): `data/*split*/trainSummary.txt`\n",
        "- Cleans the title tokens: `data/*split*/trainTitle.txt`\n",
        "- Labels the occurrences of records mentioned within the summary: `data/*split*/trainDataLabel.txt`\n",
        "- Labels the summary tokens which match a record: `data/*split*/trainSummaryLabel.txt`\n",
        "- Saves the gold summaries: `data/*split*/testOriginalSummary.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3mLPlddECyq"
      },
      "source": [
        "def preprocessData(df, chartType = 'bar_chart'):\n",
        "  complexChartTypes = []\n",
        "\n",
        "  cols, size = openMultiColumnData(df)\n",
        "  complexChartTypes.append(chartType)\n",
        "  cleanCols = [cleanAxisLabel(axis) for axis in cols]\n",
        "  dataLine = ''\n",
        "  colData = []\n",
        "  for col in df:\n",
        "      vals = df[col].values\n",
        "      cleanVals = [cleanAxisValue(str(value)) for value in vals]\n",
        "      colData.append(cleanVals)\n",
        "  # iterate through each table row\n",
        "  for m in range(0, size):\n",
        "      axisTypes = []\n",
        "      #rowData = []\n",
        "      dataLabels = []\n",
        "      for axis, n in zip(cols, range(cols.size)):\n",
        "          if is_number(axis[0]):\n",
        "              axisTypes.append('numerical')\n",
        "          else:\n",
        "              axisTypes.append('categorical')\n",
        "          value = str(df.at[m, axis])\n",
        "          cleanValue = cleanAxisValue(value)\n",
        "          #rowData.append(cleanValue)\n",
        "          record = f\"{cleanCols[n]}|{cleanValue}|{n}|{chartType}\"\n",
        "          dataLine += f'{record} '\n",
        "          dataLabels.append([0 for item in range(size)])\n",
        "\n",
        "  return dataLine"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3EYXiEUfx7w"
      },
      "source": [
        "## Generate summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiZz2TOwf-0g"
      },
      "source": [
        "def generateSummary(titleLine, tableDataLine):\n",
        "    params = argparse.Namespace(batch_size=8, \n",
        "                              beam_size=4, \n",
        "                              early_stopping=False, \n",
        "                              length_penalty=1.0, \n",
        "                              model_path='./models/aug17-80.pth',\n",
        "                              output_path='output.txt')\n",
        "    assert os.path.isfile(params.model_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # generate parser / parse parameters\n",
        "        #parser = get_parser()\n",
        "        #params = parser.parse_args()\n",
        "        reloaded = torch.load(params.model_path)\n",
        "\n",
        "        model_params = AttrDict(reloaded['params'])\n",
        "        \n",
        "\n",
        "        # update dictionary parameters\n",
        "        for name in ['src_n_words', 'tgt_n_words', 'bos_index', 'eos_index', 'pad_index', 'unk_index', 'mask_index']:\n",
        "            setattr(params, name, getattr(model_params, name))\n",
        "        # print(f'src {getattr(model_params, \"src_n_words\")}')\n",
        "        # print(f'tgt {getattr(model_params, \"tgt_n_words\")}')\n",
        "        # build dictionary / build encoder / build decoder / reload weights\n",
        "        source_dico = Dictionary(reloaded['source_dico_id2word'], reloaded['source_dico_word2id'])\n",
        "        target_dico = Dictionary(reloaded['target_dico_id2word'], reloaded['target_dico_word2id'])\n",
        "        # originalDecoder = reloaded['decoder'].copy()\n",
        "        encoder = TransformerEncoder(model_params, source_dico, with_output=False)\n",
        "        encoder.load_state_dict(reloaded['encoder'])\n",
        "        decoder = TransformerDecoder(model_params, target_dico, with_output=True)\n",
        "        decoder.load_state_dict(reloaded['decoder'])\n",
        "        # read sentences from stdin\n",
        "        table_lines = [tableDataLine]\n",
        "        title_lines = [titleLine]\n",
        "\n",
        "\n",
        "\n",
        "        #Do we have as many Titles as Diagram data?\n",
        "        assert len(title_lines) == len(table_lines)\n",
        "\n",
        "        #Opens the output file\n",
        "        outf = io.open(params.output_path, 'w', encoding='utf-8')\n",
        "\n",
        "        fillers = ['in', 'the', 'and', 'or', 'an', 'as', 'can', 'be', 'a', ':', '-',\n",
        "                  'to', 'but', 'is', 'of', 'it', 'on', '.', 'at', '(', ')', ',', 'with']\n",
        "\n",
        "        #Do for all lines incrementing with batch_size\n",
        "        for i in range(0, len(table_lines), params.batch_size):\n",
        "            # prepare batch\n",
        "\n",
        "            \"\"\"valueLengths = []\n",
        "            xLabelLengths = []\n",
        "            yLabelLengths = []\n",
        "            titleLengths = []\"\"\"\n",
        "            enc_x1_ids = []\n",
        "            enc_x2_ids = []\n",
        "            enc_x3_ids = []\n",
        "            enc_x4_ids = []\n",
        "            for table_line, title_line in zip(table_lines[i:i + params.batch_size], title_lines[i:i + params.batch_size]):\n",
        "                record_seq = [each.split('|') for each in table_line.split()]\n",
        "                assert all([len(x) == 4 for x in record_seq])\n",
        "\n",
        "                enc_x1_ids.append(torch.LongTensor([source_dico.index(x[0]) for x in record_seq]))\n",
        "                enc_x2_ids.append(torch.LongTensor([source_dico.index(x[1]) for x in record_seq]))\n",
        "                enc_x3_ids.append(torch.LongTensor([source_dico.index(x[2]) for x in record_seq]))\n",
        "                enc_x4_ids.append(torch.LongTensor([source_dico.index(x[3]) for x in record_seq]))\n",
        "\n",
        "                xLabel = record_seq[1][0].split('_')\n",
        "                yLabel = record_seq[0][0].split('_')\n",
        "                \"\"\"cleanXLabel = len([item for item in xLabel if item not in fillers])\n",
        "                cleanYLabel = len([item for item in yLabel if item not in fillers])\n",
        "                cleanTitle = len([word for word in title_line.split() if word not in fillers])\n",
        "\n",
        "                xLabelLengths.append(cleanXLabel)\n",
        "                yLabelLengths.append(cleanYLabel)\n",
        "                titleLengths.append(cleanTitle)\n",
        "                valueLengths.append(round(len(record_seq)/2))\"\"\"\n",
        "\n",
        "            enc_xlen = torch.LongTensor([len(x) + 2 for x in enc_x1_ids])\n",
        "            enc_x1 = torch.LongTensor(enc_xlen.max().item(), enc_xlen.size(0)).fill_(params.pad_index)\n",
        "            enc_x1[0] = params.eos_index\n",
        "            enc_x2 = torch.LongTensor(enc_xlen.max().item(), enc_xlen.size(0)).fill_(params.pad_index)\n",
        "            enc_x2[0] = params.eos_index\n",
        "            enc_x3 = torch.LongTensor(enc_xlen.max().item(), enc_xlen.size(0)).fill_(params.pad_index)\n",
        "            enc_x3[0] = params.eos_index\n",
        "            enc_x4 = torch.LongTensor(enc_xlen.max().item(), enc_xlen.size(0)).fill_(params.pad_index)\n",
        "            enc_x4[0] = params.eos_index\n",
        "\n",
        "            for j, (s1,s2,s3,s4) in enumerate(zip(enc_x1_ids, enc_x2_ids, enc_x3_ids, enc_x4_ids)):\n",
        "                if enc_xlen[j] > 2:  # if sentence not empty\n",
        "                    enc_x1[1:enc_xlen[j] - 1, j].copy_(s1)\n",
        "                    enc_x2[1:enc_xlen[j] - 1, j].copy_(s2)\n",
        "                    enc_x3[1:enc_xlen[j] - 1, j].copy_(s3)\n",
        "                    enc_x4[1:enc_xlen[j] - 1, j].copy_(s4)\n",
        "                enc_x1[enc_xlen[j] - 1, j] = params.eos_index\n",
        "                enc_x2[enc_xlen[j] - 1, j] = params.eos_index\n",
        "                enc_x3[enc_xlen[j] - 1, j] = params.eos_index\n",
        "                enc_x4[enc_xlen[j] - 1, j] = params.eos_index\n",
        "\n",
        "            enc_x1 = enc_x1.cuda()\n",
        "            enc_x2 = enc_x2.cuda()\n",
        "            enc_x3 = enc_x3.cuda()\n",
        "            enc_x4 = enc_x4.cuda()\n",
        "            enc_xlen = enc_xlen.cuda()\n",
        "\n",
        "            # encode source batch and translate it\n",
        "            encoder_output = encoder('fwd', x1=enc_x1, x2=enc_x2, x3=enc_x3, x4=enc_x4, lengths=enc_xlen)\n",
        "            encoder_output = encoder_output.transpose(0, 1)\n",
        "\n",
        "            max_len = 602\n",
        "            if params.beam_size <= 1:\n",
        "                decoded, dec_lengths = decoder.generate(encoder_output, enc_xlen, max_len=max_len)\n",
        "            elif params.beam_size > 1:\n",
        "                decoded, dec_lengths = decoder.generate_beam(encoder_output, enc_xlen, params.beam_size, \n",
        "                                                params.length_penalty, params.early_stopping, max_len=max_len)\n",
        "\n",
        "            for j in range(decoded.size(1)):\n",
        "                # remove delimiters\n",
        "                sent = decoded[:, j]\n",
        "                delimiters = (sent == params.eos_index).nonzero().view(-1)\n",
        "                assert len(delimiters) >= 1 and delimiters[0].item() == 0\n",
        "                sent = sent[1:] if len(delimiters) == 1 else sent[1:delimiters[1]]\n",
        "                # print(sent)\n",
        "                # output translation\n",
        "                # source = table_lines[i + j].strip()\n",
        "                # print(source)\n",
        "                tokens = []\n",
        "                for k in range(len(sent)):\n",
        "                    ids = sent[k].item()\n",
        "                    #if ids in removedDict:\n",
        "                    #    print('index error')\n",
        "                    word = target_dico[ids]\n",
        "                    tokens.append(word)\n",
        "                target = \" \".join(tokens)\n",
        "                sys.stderr.write(\"%i / %i: %s\\n\" % (i + j, len(table_lines), target))\n",
        "                outf.write(target + \"\\n\")\n",
        "        outf.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snkQce3gD89h"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "T_XIioIi_eob",
        "outputId": "2600e7eb-b7bd-4131-9609-1c05a45eeb03"
      },
      "source": [
        "testTitles = ['Facebook: number of monthly active users worldwide 2008-2019','National Basketball Association all-time scoring leaders 1946-2020','Instagram accounts with the most followers worldwide 2020']\n",
        "\n",
        "\n",
        "# Import pandas library\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "TESTDATA = StringIO(\"\"\"Country,Average number of children per woman\n",
        "Singapore,1.38\n",
        "Republic of Korea,1.44\n",
        "United Arab Emirates,1.45\n",
        "Puerto Rico,1.45\n",
        "Bosnia and Herzegovina,1.47\n",
        "Saint Lucia,1.48\n",
        "Greece,1.5\n",
        "Cyprus,1.51\n",
        "Italy,1.51\n",
        "Republic of Moldova,1.52\n",
        "\"China, Taiwan Province of China\",1.53\n",
        "Albania,1.53\n",
        "Mauritius,1.54\n",
        "Thailand,1.54\n",
        "Qatar,1.56\n",
        "Nepal,1.56\n",
        "Croatia,1.56\n",
        "Japan,1.57\n",
        "Serbia,1.57\n",
        "Brazil,1.58\n",
        "North Macedonia,1.58\n",
        "Brunei Darussalam,1.59\n",
        "Portugal,1.59\n",
        "Spain,1.59\n",
        "Canada,1.59\n",
        "    \"\"\")\n",
        "\n",
        "df = pd.read_csv(TESTDATA, sep=\",\")\n",
        "  \n",
        "# print dataframe.\n",
        "cleanedTitle = clean_title('Countries with the lowest fertility rate globally 2050-2055')\n",
        "dataline = preprocessData(df)\n",
        "\n",
        "generateSummary(cleanedTitle, dataline)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11198\n",
            "11198\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-841f29c595a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdataline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mgenerateSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanedTitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-0588a96abcf1>\u001b[0m in \u001b[0;36mgenerateSummary\u001b[0;34m(titleLine, tableDataLine)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# encode source batch and translate it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fwd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_x3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_x4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_xlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7bc99238b247>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fwd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7bc99238b247>\u001b[0m in \u001b[0;36mfwd\u001b[0;34m(self, x1, x2, x3, x4, lengths)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mslen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAOQGxjXMp2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2928e854-213f-4e62-d764-2683b9b1ff6f"
      },
      "source": [
        "% cd gdrive/MyDrive/BA-Code/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/BA-Code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EGmseERM1Ym",
        "outputId": "6c8d2af3-f43e-4900-fc5c-92eadc832739"
      },
      "source": [
        "ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chart_2_text_reduced.ipynb  \u001b[0m\u001b[01;34mmodels\u001b[0m/  output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFGtgPJFNbk1",
        "outputId": "b912dea2-1f9e-45d5-c511-585e4e035285"
      },
      "source": [
        "cd BA-Code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'BA-Code'\n",
            "/content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsGO47nxGcD4",
        "outputId": "b87b35a9-7986-46a0-ec6e-d35bc78dd0dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "IHNIn27nNgx9",
        "outputId": "2be7984a-c6bf-4d34-d310-967270b66a29"
      },
      "source": [
        "! pip install torch==1.5.0 torchvision==0.6.0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.5.0\n",
            "  Downloading torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0 MB 10 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0\n",
            "  Downloading torchvision-0.6.0-cp37-cp37m-manylinux1_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.0 torchvision-0.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}