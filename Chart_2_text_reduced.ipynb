{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chart_2_text_reduced.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "B92qnMW1Oqmi",
        "T1XNOGTxJO4p",
        "XB1A-1yiEAtx"
      ],
      "authorship_tag": "ABX9TyNFq7sSV4lL70vSoxA8EWPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EliasKng/BT-Code/blob/master/Chart_2_text_reduced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3mHL9GTjbPL"
      },
      "source": [
        "#Chart-2-text-reduced\n",
        "\n",
        "This notebook will provide the functionality of chart-to-text, however, for single value inputs.\n",
        "\n",
        "So it will do the data-preparation and then put the values into the model and return the summary for the chart.\n",
        "\n",
        "The goal is to provide a function:\n",
        "\n",
        " **createSummary(chartData: ChartData): string**\n",
        "\n",
        "where the returned string is the summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EceSAKtf8Udu"
      },
      "source": [
        "## Startup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B92qnMW1Oqmi"
      },
      "source": [
        "### Installations & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaLPeoJzSelX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e454ad65-9de3-46db-c2a2-21bc642d5eef"
      },
      "source": [
        "! pip install -U spacy\n",
        "! python3 -m spacy download en_core_web_md\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from typing import List\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting en-core-web-md==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 45.7 MB 139 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1XNOGTxJO4p"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9foa_vyJJN_3"
      },
      "source": [
        "def word_tokenize(string: str, language: str = \"english\") -> List[str]:\n",
        "    \"\"\"tokenizes a given string into a list of substrings.\n",
        "\n",
        "    :param string: String to tokenize.\n",
        "    :param language: Language. Either one of ``english'' or ``german''.\n",
        "    \"\"\"\n",
        "    if language not in [\"english\", \"german\"]:\n",
        "        raise ValueError(\"language argument has to be either ``english'' or ``german''\")\n",
        "\n",
        "    # excessive whitespaces\n",
        "    string = re.sub(r\"\\s+\", \" \", string)\n",
        "\n",
        "    # some unicode characters\n",
        "    string = string.replace(\"’\", \"'\")\n",
        "    string = string.replace(\"”\", '\"')\n",
        "    string = string.replace(\"“\", '\"')\n",
        "\n",
        "    # floating point (e.g., 1.3 => 1.3)\n",
        "    string = re.sub(r\"(\\d+)\\.(\\d+)\", r\"\\g<1>._\\g<2>\", string)\n",
        "\n",
        "    # percentage (e.g., below.500 => below .500)\n",
        "    string = re.sub(r\"(\\w+)\\.(\\d+)\", r\"\\g<1> ._\\g<2>\", string)\n",
        "\n",
        "    # end of quote\n",
        "    string = string.replace(\".``\", \". ``\")\n",
        "\n",
        "    # number with apostrophe (e.g. '90)\n",
        "    string = re.sub(r\"\\s'(\\d+)\", r\"' \\g<1>\", string)\n",
        "\n",
        "    # names with Initials (e.g. C. J. Miles)\n",
        "    string = re.sub(r\"(^|\\s)(\\w)\\. (\\w)\\.\", r\"\\g<1>\\g<2>._ \\g<3>._\", string)\n",
        "\n",
        "    # some dots\n",
        "    string = string.replace(\"..\", \" ..\")\n",
        "\n",
        "    # names with apostrophe => expands temporarily\n",
        "    string = re.sub(r\"\\w+'(?!d|s|ll|t|re|ve|\\s)\", r\"\\g<0>_\", string)\n",
        "\n",
        "    # win-loss scores (German notation seems to be XX:YY, but this is also the time format,\n",
        "    # and the times are not tokenized in the original RotoWire. So we manually handle XX:YY\n",
        "    # expression.\n",
        "    string = re.sub(r\"(\\d+)-(\\d+)\", r\"\\g<1> - \\g<2>\", string)\n",
        "\n",
        "    # actual tokenization\n",
        "    tokenized = nltk.word_tokenize(string, language=language)\n",
        "\n",
        "    joined = \" \".join(tokenized)\n",
        "    # shrink expanded name-with-apostrophe expressions\n",
        "    joined = joined.replace(\"'_\", \"'\")\n",
        "    # shrink expanded name-with-initial expressions\n",
        "    joined = joined.replace(\"._\", \".\")\n",
        "    tokenized = joined.split(\" \")\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "def cleanAxisLabel(label):\n",
        "    cleanLabel = re.sub('\\s', '_', label)\n",
        "    cleanLabel = cleanLabel.replace('%', '').replace('*', '')\n",
        "    return cleanLabel\n",
        "  \n",
        "def cleanAxisValue(value):\n",
        "    #print(value)\n",
        "    if value == '-' or value == 'nan':\n",
        "        return '0'\n",
        "    cleanValue = re.sub('\\s', '_', value)\n",
        "    cleanValue = cleanValue.replace('|', '').replace(',', '').replace('%', '').replace('*', '')\n",
        "    return cleanValue\n",
        "\n",
        "def is_number(string):\n",
        "    try:\n",
        "        float(string)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def openMultiColumnData(df):\n",
        "    cols = df.columns\n",
        "    size = df.shape[0]\n",
        "    return cols, size\n",
        "  \n",
        "def getSubject(titleTokens, nerEntities):\n",
        "    fillers = ['in', 'the', 'and', 'or', 'an', 'as', 'can', 'be', 'a', ':', '-',\n",
        "              'to', 'but', 'is', 'of', 'it', 'on', '.', 'at', '(', ')', ',', ';']\n",
        "    entities = {}\n",
        "    entities['Subject'] = []\n",
        "    entities['Date'] = []\n",
        "    # manually find dates, it performs better than using NER\n",
        "    for word in titleTokens:\n",
        "        if word.isnumeric():\n",
        "            if len(word) > 3:\n",
        "                entities['Date'].append(word)\n",
        "        elif word.replace('/', '').isnumeric():\n",
        "            word = word.split('/')[0]\n",
        "            if len(word) > 3:\n",
        "                entities['Date'].append(word)\n",
        "        elif word.replace('-', '').isnumeric():\n",
        "            word = word.split('-')[0]\n",
        "            if len(word) > 3:\n",
        "                entities['Date'].append(word)\n",
        "    # get named entites from title\n",
        "    for X in nerEntities:\n",
        "        if X.label_ == 'GPE' or X.label_ == 'ORG' or X.label_ == 'NORP' or X.label_ == 'LOC':\n",
        "            cleanSubject = [word for word in X.text.split() if word.isalpha() and word not in fillers]\n",
        "            if len(cleanSubject) > 0:\n",
        "                entities['Subject'].append(' '.join(cleanSubject))\n",
        "        if len(entities['Date']) < 1:\n",
        "            if X.label_ == 'DATE':\n",
        "                if X.text.isnumeric():\n",
        "                    entities['Date'].append(X.text)\n",
        "    # guess subject if NER doesn't find one\n",
        "    if len(entities['Subject']) == 0:\n",
        "        uppercaseWords = [word for word in titleTokens if word[0].isupper()]\n",
        "        if len(uppercaseWords) > 1:\n",
        "            guessedSubject = ' '.join(uppercaseWords[1:])\n",
        "        else:\n",
        "            guessedSubject = uppercaseWords[0]\n",
        "        entities['Subject'].append(guessedSubject)\n",
        "    # print(entities['Date'])\n",
        "    cleanTitle = [titleWord for titleWord in titleTokens if titleWord.lower() not in fillers]\n",
        "    return entities, cleanTitle\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpRK-rD676c4"
      },
      "source": [
        "## Cleaning Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB1A-1yiEAtx"
      },
      "source": [
        "### Titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA3MeXhOjXTz"
      },
      "source": [
        "def clean_title(title):\n",
        "  cleanedTitle = word_tokenize(title)\n",
        "  # replace (2009 - 2016) with (2009 to 2016)\n",
        "  lastTokens = cleanedTitle[-3:]\n",
        "  if lastTokens[1] == '-' and lastTokens[0].isnumeric() and lastTokens[2].isnumeric():\n",
        "    cleanedTitle[-2] = 'to'\n",
        "  cleanedTitle = ' '.join(cleanedTitle).replace('*', '')\n",
        "  return cleanedTitle"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7dA-aZzED5b"
      },
      "source": [
        "###Preprocessing\n",
        "- Converts data tables into a sequence of records (taken as input by the model): `data/*split*/trainData.txt`\n",
        "- Cleans summary tokens and substitutes any possible tokens with data variables(e.g., 2018 -> templateValue[0][0]): `data/*split*/trainSummary.txt`\n",
        "- Cleans the title tokens: `data/*split*/trainTitle.txt`\n",
        "- Labels the occurrences of records mentioned within the summary: `data/*split*/trainDataLabel.txt`\n",
        "- Labels the summary tokens which match a record: `data/*split*/trainSummaryLabel.txt`\n",
        "- Saves the gold summaries: `data/*split*/testOriginalSummary.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3mLPlddECyq"
      },
      "source": [
        "def preprocessData(df, title, chartType = 'bar_chart'):\n",
        "  # \"\"\"\n",
        "  # df is an df containing the data\n",
        "  # title is a string which is the cleanedTitle from clean_title()\n",
        "  # chart_type is a string: ('line_chart' | 'bar_chart')\n",
        "  # \"\"\"\n",
        "\n",
        "  # cols = df.columns\n",
        "  # size = df.shape[0]\n",
        "  # cleanCols = [cleanAxisLabel(axis) for axis in cols]\n",
        "  \n",
        "  # dataLine = ''\n",
        "  # colData = []\n",
        "  \n",
        "  # for col in df:\n",
        "  #   vals = df[col].values\n",
        "  #   cleanVals = [cleanAxisValue(str(value)) for value in vals]\n",
        "  #   colData.append(cleanVals)\n",
        "  \n",
        "  # for m in range(0,size):\n",
        "  #   axisTypes = []\n",
        "  #   records = []\n",
        "  #   dataLabels = []\n",
        "  #   for axis, n in zip(cols, range(cols.size)):\n",
        "  #     if is_number(axis[0]):\n",
        "  #       axisTypes.append('numerical')\n",
        "  #     else:\n",
        "  #       axisTypes.append('categorical')\n",
        "  #     value = str(df.at[m, axis])\n",
        "  #     cleanValue = cleanAxisValue(value)\n",
        "  #     record = f\"{cleanCols[n]}|{cleanValue}|{n}|{chartType}\"\n",
        "  #     dataLine += f'{record} '\n",
        "  #     dataLabels.append([0 for item in range(size)])\n",
        "  dataArr = []\n",
        "  dataLabelArr = []\n",
        "  summaryArr = []\n",
        "  summaryLabelArr = []\n",
        "  labelList = []\n",
        "  titleArr = []\n",
        "  oldSummaryArr = []\n",
        "\n",
        "  dataRatioArr = []\n",
        "  captionRatioArr = []\n",
        "\n",
        "  #assert len(captionFiles) == len(dataFiles) == len(titleFiles)\n",
        "  #print(len(captionFiles), len(dataFiles), len(titleFiles))\n",
        "  # may implemented seperately to avoid accidentally ignoring the word rather than month\n",
        "  months = ['january', 'february', 'march', 'april', 'june', 'july', 'august', 'september', 'november', 'december']\n",
        "\n",
        "  years = [str(i) for i in range(1850, 2055)]\n",
        "\n",
        "  fillers = ['in', 'the', 'and', 'or', 'an', 'as', 'can', 'be', 'a', ':', '-',\n",
        "            'to', 'but', 'is', 'of', 'it', 'on', '.', 'at', '(', ')', ',', ';']\n",
        "  \n",
        "  numbers = ['percent', 'percentage', '%', 'hundred', 'thousand', 'million', 'billion', 'trillion',\n",
        "            'hundreds', 'thousands', 'millions', 'billions', 'trillions']\n",
        "  \n",
        "  positiveTrends = ['increased', 'increase', 'increasing', 'grew', 'growing', 'rose', 'rising', 'gained', 'gaining']\n",
        "  negativeTrends = ['decreased', 'decrease', 'decreasing', 'shrank', 'shrinking', 'fell', 'falling', 'dropped',\n",
        "                    'dropping']\n",
        "  \n",
        "  simpleChartTypes = []\n",
        "  complexChartTypes = []\n",
        "\n",
        "  caption = ''\n",
        "  cols, size = openMultiColumnData(df)\n",
        "  complexChartTypes.append(chartType)\n",
        "  cleanCols = [cleanAxisLabel(axis) for axis in cols]\n",
        "  dataLine = ''\n",
        "  summaryLabelLine = \"\"\n",
        "  colData = []\n",
        "  for col in df:\n",
        "      vals = df[col].values\n",
        "      cleanVals = [cleanAxisValue(str(value)) for value in vals]\n",
        "      colData.append(cleanVals)\n",
        "  # iterate through each table row\n",
        "  for m in range(0, size):\n",
        "      axisTypes = []\n",
        "      #rowData = []\n",
        "      records = []\n",
        "      dataLabels = []\n",
        "      for axis, n in zip(cols, range(cols.size)):\n",
        "          if is_number(axis[0]):\n",
        "              axisTypes.append('numerical')\n",
        "          else:\n",
        "              axisTypes.append('categorical')\n",
        "          value = str(df.at[m, axis])\n",
        "          cleanValue = cleanAxisValue(value)\n",
        "          #rowData.append(cleanValue)\n",
        "          record = f\"{cleanCols[n]}|{cleanValue}|{n}|{chartType}\"\n",
        "          dataLine += f'{record} '\n",
        "          dataLabels.append([0 for item in range(size)])\n",
        "  captionSentences = caption.split(' . ')\n",
        "  if len(captionSentences) >= 4:\n",
        "      trimmedCaption = (' . ').join(captionSentences[0:3]) + ' .\\n'\n",
        "  else:\n",
        "      trimmedCaption = (' . ').join(captionSentences)\n",
        "  captionTokens = trimmedCaption.split()\n",
        "\n",
        "  labelMap = []\n",
        "  captionMatchCount = 0\n",
        "  doc = nlp(title)\n",
        "  entities, cleanTitle = getSubject(title.split(), doc.ents)\n",
        "\n",
        "  parallelData = []\n",
        "  for token, m in zip(captionTokens, range(0, len(captionTokens))):\n",
        "      # check for duplicates before token replacement\n",
        "      if m < len(captionTokens) - 1:\n",
        "          if captionTokens[m] == captionTokens[m + 1]:\n",
        "              captionTokens.pop(m + 1)\n",
        "      if token.lower() not in fillers:\n",
        "          # find labels for summary tokens, call function to replace token with template\n",
        "          tokenBool, newToken = compareMultiColumnToken(captionTokens, m, cleanTitle, colData,\n",
        "                                                        cleanCols, entities)\n",
        "          if tokenBool == 1:\n",
        "              #print(newToken)\n",
        "              captionTokens[m] = newToken\n",
        "              captionMatchCount += 1\n",
        "      else:\n",
        "          tokenBool = 0\n",
        "      # check for duplicates after token replacement\n",
        "      if m > 0:\n",
        "          if captionTokens[m - 1] == captionTokens[m]:\n",
        "              captionTokens.pop(m)\n",
        "          # check if last token was an un-templated month\n",
        "          elif captionTokens[m].lower() in months or captionTokens[m] == 'May':\n",
        "              captionTokens.pop(m)\n",
        "      else:\n",
        "          print(token)\n",
        "          tokenBool = 0\n",
        "      labelMap.append(str(tokenBool))\n",
        "  assert len(captionTokens) == len(labelMap)\n",
        "  # replace tokens with their parallel templates if they exist\n",
        "  # ex: in 2019 sales was 300 million -> in templateXValue[max] sales was templateYValue[idxmax(x)] million\n",
        "  if len(parallelData) > 0:\n",
        "      for item in parallelData:\n",
        "          template = item[0]\n",
        "          axis = item[1]\n",
        "          tokenIndex = item[2]\n",
        "          try:\n",
        "              labelMap[tokenIndex] = '1'\n",
        "              captionTokens[tokenIndex] = template\n",
        "          except IndexError:\n",
        "              # TODO find out if this means that any time a pop occurs the replacement is misaligned,\n",
        "              # maybe track the # of pops and subtract that from tokenIndex\n",
        "              # this happens twice due to popping values and changing length of list\n",
        "              print('index error')\n",
        "              tokenIndex = len(labelMap) - 1\n",
        "              labelMap[tokenIndex] = '1'\n",
        "              captionTokens[tokenIndex] = template\n",
        "  # check for sentences containing a delta value\n",
        "  newSentences = []\n",
        "  cleanSentences = ' '.join(captionTokens).split(' . ')\n",
        "  for sentence, sentIdx in zip(cleanSentences, range(len(cleanSentences))):\n",
        "      scaleIndicator = False\n",
        "      trendIndicator = False\n",
        "      newSentence = []\n",
        "      for token, tokenIdx in zip(sentence.split(), range(len(sentence))):\n",
        "          if token == 'templateScale':\n",
        "              try:\n",
        "                  scale = captionSentences[sentIdx].split()[tokenIdx]\n",
        "                  if scale in numbers:\n",
        "                      scaleIndicator = True\n",
        "              except:\n",
        "                  print('scale err')\n",
        "          if token.lower() in positiveTrends:\n",
        "              token = 'templatePositiveTrend'\n",
        "              trendIndicator = True\n",
        "          elif token.lower() in negativeTrends:\n",
        "              token = 'templateNegativeTrend'\n",
        "              trendIndicator = True\n",
        "          # if there is an unlabelled numeric token in a sentence containing a trend word, assume that token is a delta between two values\n",
        "          \"\"\"\n",
        "          if trendIndicator:\n",
        "              if token not in years:\n",
        "                  if is_number(token):\n",
        "                      sentenceTemplates = [token for token in sentence.split() if 'template' in token]\n",
        "                      xCount = {token for token in sentenceTemplates if 'templateXValue' in token}\n",
        "                      yCount = {token for token in sentenceTemplates if 'templateYValue' in token}\n",
        "                      # also compare 1 x and 1 y s\n",
        "                      if len(xCount) == 2 or len(yCount) == 2 or (len(xCount) == 1 and len(yCount) == 1):\n",
        "                          values, indices = getTemplateValues(xCount, yCount, xValueArr, yValueArr)\n",
        "                          if len(values) > 1:\n",
        "                              print(token, tokenIdx)\n",
        "                              print(sentence)\n",
        "                              print(xValueArr)\n",
        "                              print(yValueArr)\n",
        "                              print(xCount, values)\n",
        "                              print(scale)\n",
        "                              if scaleIndicator and (scale == 'percent' or scale == 'percentage'):\n",
        "                                  valueDiff = abs((float(values[1]) - float(values[0]) / float(values[0])) * 100)\n",
        "                                  rounded1 = abs(normal_round(valueDiff))\n",
        "                                  rounded2 = abs(normal_round(valueDiff, 1))\n",
        "                                  print(f'original: {token}, diff:{valueDiff} rounded:{rounded1, rounded2}')\n",
        "                              else:\n",
        "                                  valueDiff = abs(float(values[0]) - float(values[1]))\n",
        "                                  rounded1 = abs(normal_round(valueDiff))\n",
        "                                  rounded2 = abs(normal_round(valueDiff, 1))\n",
        "                                  print(f'original: {token}, diff:{valueDiff} rounded:{rounded1, rounded2}')\n",
        "                              if rounded1 == float(token) or rounded2 == float(token) or valueDiff == float(token):\n",
        "                                  token = f'templateDelta[{indices[0]},{indices[1]}]'\n",
        "                                  print('DELTA')\"\"\"\n",
        "          newSentence.append(token)\n",
        "      newSentences.append(' '.join(newSentence))\n",
        "  assert len(captionTokens) == len(labelMap)\n",
        "  dataLabelLine = (' ').join([' '.join([str(item) for item in column]) for column in dataLabels])\n",
        "  labelCount = sum([len(column) for column in dataLabels])\n",
        "  assert len(dataLabelLine.split()) == labelCount\n",
        "  dataMatchCount = sum([sum(column) for column in dataLabels])\n",
        "  dataRatio = round(dataMatchCount / labelCount, 2)\n",
        "  #captionRatio = round(captionMatchCount / len(captionTokens), 2)\n",
        "\n",
        "  for col in colData:\n",
        "      assert labelCount/len(colData) == len(col)\n",
        "  dataRatioArr.append(dataRatio)\n",
        "  #captionRatioArr.append(captionRatio)\n",
        "  summaryLabelLine = (' ').join(labelMap)\n",
        "  assert len(captionTokens) == len(summaryLabelLine.split())\n",
        "  newCaption = (' . ').join(newSentences)\n",
        "  oldSummaryArr.append(trimmedCaption)\n",
        "  labelList.append(labelMap)\n",
        "  dataArr.append(dataLine)\n",
        "  dataLabelArr.append(dataLabelLine)\n",
        "  summaryArr.append(newCaption)\n",
        "  summaryLabelArr.append(summaryLabelLine)\n",
        "  titleArr.append(title)\n",
        "  print('dataLine')\n",
        "  print(dataLine)\n",
        "  \n",
        "  print(dataArr)\n",
        "  print(dataLabelArr)\n",
        "  print(summaryArr)\n",
        "  print(summaryLabelArr)\n",
        "  print(titleArr)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snkQce3gD89h"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_XIioIi_eob",
        "outputId": "3eade9b3-d597-463e-88cc-fb42e8110fb4"
      },
      "source": [
        "testTitles = ['Facebook: number of monthly active users worldwide 2008-2019','National Basketball Association all-time scoring leaders 1946-2020','Instagram accounts with the most followers worldwide 2020']\n",
        "\n",
        "\n",
        "# Import pandas library\n",
        "import pandas as pd\n",
        "  \n",
        "# initialize list of lists\n",
        "data = [['White', 457], ['Black', 223], ['Hispanic', 179], ['Other', 44], ['Unknown', 84]]\n",
        "  \n",
        "# Create the pandas DataFrame\n",
        "df = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
        "  \n",
        "# print dataframe.\n",
        "print(df)\n",
        "\n",
        "preprocessData(df, 'This is just a testTitle')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Name  Age\n",
            "0     White  457\n",
            "1     Black  223\n",
            "2  Hispanic  179\n",
            "3     Other   44\n",
            "4   Unknown   84\n",
            "captionMatch\n",
            "0\n",
            "dataMatch\n",
            "0\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5spk_IFTs2lH"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}